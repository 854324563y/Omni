[2025-03-27 12:43:28 root] (main.py 258): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log/MMLU/llama-13b-w4a6', save_dir=None, resume='./log/llama-13b-w4a6/omni_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, aug_loss=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-03-27 12:50:46 root] (main.py 324): INFO === start quantization ===
[2025-03-27 12:50:46 root] (main.py 330): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-27 12:50:46 root] (omniquant.py 50): INFO Starting ...
[2025-03-27 12:50:49 root] (omniquant.py 193): INFO === Start quantize layer 0 ===
[2025-03-27 12:50:50 root] (omniquant.py 193): INFO === Start quantize layer 1 ===
[2025-03-27 12:50:51 root] (omniquant.py 193): INFO === Start quantize layer 2 ===
[2025-03-27 12:50:52 root] (omniquant.py 193): INFO === Start quantize layer 3 ===
[2025-03-27 12:50:53 root] (omniquant.py 193): INFO === Start quantize layer 4 ===
[2025-03-27 12:50:54 root] (omniquant.py 193): INFO === Start quantize layer 5 ===
[2025-03-27 12:50:55 root] (omniquant.py 193): INFO === Start quantize layer 6 ===
[2025-03-27 12:50:56 root] (omniquant.py 193): INFO === Start quantize layer 7 ===
[2025-03-27 12:50:57 root] (omniquant.py 193): INFO === Start quantize layer 8 ===
[2025-03-27 12:50:59 root] (omniquant.py 193): INFO === Start quantize layer 9 ===
[2025-03-27 12:51:00 root] (omniquant.py 193): INFO === Start quantize layer 10 ===
[2025-03-27 12:51:01 root] (omniquant.py 193): INFO === Start quantize layer 11 ===
[2025-03-27 12:51:02 root] (omniquant.py 193): INFO === Start quantize layer 12 ===
[2025-03-27 12:51:03 root] (omniquant.py 193): INFO === Start quantize layer 13 ===
[2025-03-27 12:51:04 root] (omniquant.py 193): INFO === Start quantize layer 14 ===
[2025-03-27 12:51:05 root] (omniquant.py 193): INFO === Start quantize layer 15 ===
[2025-03-27 12:51:07 root] (omniquant.py 193): INFO === Start quantize layer 16 ===
[2025-03-27 12:51:08 root] (omniquant.py 193): INFO === Start quantize layer 17 ===
[2025-03-27 12:51:09 root] (omniquant.py 193): INFO === Start quantize layer 18 ===
[2025-03-27 12:51:10 root] (omniquant.py 193): INFO === Start quantize layer 19 ===
[2025-03-27 12:51:11 root] (omniquant.py 193): INFO === Start quantize layer 20 ===
[2025-03-27 12:51:12 root] (omniquant.py 193): INFO === Start quantize layer 21 ===
[2025-03-27 12:51:14 root] (omniquant.py 193): INFO === Start quantize layer 22 ===
[2025-03-27 12:51:15 root] (omniquant.py 193): INFO === Start quantize layer 23 ===
[2025-03-27 12:51:16 root] (omniquant.py 193): INFO === Start quantize layer 24 ===
[2025-03-27 12:51:17 root] (omniquant.py 193): INFO === Start quantize layer 25 ===
[2025-03-27 12:51:18 root] (omniquant.py 193): INFO === Start quantize layer 26 ===
[2025-03-27 12:51:19 root] (omniquant.py 193): INFO === Start quantize layer 27 ===
[2025-03-27 12:51:20 root] (omniquant.py 193): INFO === Start quantize layer 28 ===
[2025-03-27 12:51:21 root] (omniquant.py 193): INFO === Start quantize layer 29 ===
[2025-03-27 12:51:22 root] (omniquant.py 193): INFO === Start quantize layer 30 ===
[2025-03-27 12:51:23 root] (omniquant.py 193): INFO === Start quantize layer 31 ===
[2025-03-27 12:51:24 root] (omniquant.py 193): INFO === Start quantize layer 32 ===
[2025-03-27 12:51:25 root] (omniquant.py 193): INFO === Start quantize layer 33 ===
[2025-03-27 12:51:26 root] (omniquant.py 193): INFO === Start quantize layer 34 ===
[2025-03-27 12:51:27 root] (omniquant.py 193): INFO === Start quantize layer 35 ===
[2025-03-27 12:51:28 root] (omniquant.py 193): INFO === Start quantize layer 36 ===
[2025-03-27 12:51:30 root] (omniquant.py 193): INFO === Start quantize layer 37 ===
[2025-03-27 12:51:31 root] (omniquant.py 193): INFO === Start quantize layer 38 ===
[2025-03-27 12:51:32 root] (omniquant.py 193): INFO === Start quantize layer 39 ===
[2025-03-27 12:51:33 root] (main.py 353): INFO 46.78128361701965
[2025-03-27 14:59:20 root] (main.py 155): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.4297520661157025, 'acc_stderr': 0.04519082021319773, 'acc_norm': 0.5454545454545454, 'acc_norm_stderr': 0.04545454545454548}, 'hendrycksTest-high_school_biology': {'acc': 0.2967741935483871, 'acc_stderr': 0.025988500792411894, 'acc_norm': 0.29354838709677417, 'acc_norm_stderr': 0.02590608702131929}, 'hendrycksTest-college_physics': {'acc': 0.22549019607843138, 'acc_stderr': 0.041583075330832865, 'acc_norm': 0.24509803921568626, 'acc_norm_stderr': 0.04280105837364395}, 'hendrycksTest-college_biology': {'acc': 0.2708333333333333, 'acc_stderr': 0.03716177437566016, 'acc_norm': 0.2708333333333333, 'acc_norm_stderr': 0.03716177437566017}, 'hendrycksTest-electrical_engineering': {'acc': 0.32413793103448274, 'acc_stderr': 0.03900432069185555, 'acc_norm': 0.33793103448275863, 'acc_norm_stderr': 0.03941707632064889}, 'hendrycksTest-high_school_physics': {'acc': 0.25165562913907286, 'acc_stderr': 0.035433042343899844, 'acc_norm': 0.2582781456953642, 'acc_norm_stderr': 0.035737053147634576}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2619047619047619, 'acc_stderr': 0.022644212615525208, 'acc_norm': 0.2857142857142857, 'acc_norm_stderr': 0.023266512213730564}, 'hendrycksTest-formal_logic': {'acc': 0.2777777777777778, 'acc_stderr': 0.04006168083848876, 'acc_norm': 0.2698412698412698, 'acc_norm_stderr': 0.039701582732351734}, 'hendrycksTest-human_aging': {'acc': 0.3273542600896861, 'acc_stderr': 0.031493846709941306, 'acc_norm': 0.24663677130044842, 'acc_norm_stderr': 0.02893041312091088}, 'hendrycksTest-jurisprudence': {'acc': 0.3055555555555556, 'acc_stderr': 0.044531975073749834, 'acc_norm': 0.4166666666666667, 'acc_norm_stderr': 0.04766075165356461}, 'hendrycksTest-moral_scenarios': {'acc': 0.24916201117318434, 'acc_stderr': 0.014465893829859936, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.01489339173524959}, 'hendrycksTest-astronomy': {'acc': 0.39473684210526316, 'acc_stderr': 0.03977749934622073, 'acc_norm': 0.4342105263157895, 'acc_norm_stderr': 0.04033565667848319}, 'hendrycksTest-conceptual_physics': {'acc': 0.2978723404255319, 'acc_stderr': 0.029896145682095462, 'acc_norm': 0.24680851063829787, 'acc_norm_stderr': 0.028185441301234095}, 'hendrycksTest-college_chemistry': {'acc': 0.26, 'acc_stderr': 0.04408440022768078, 'acc_norm': 0.33, 'acc_norm_stderr': 0.04725815626252605}, 'hendrycksTest-human_sexuality': {'acc': 0.46564885496183206, 'acc_stderr': 0.043749285605997376, 'acc_norm': 0.3435114503816794, 'acc_norm_stderr': 0.04164976071944878}, 'hendrycksTest-sociology': {'acc': 0.2885572139303483, 'acc_stderr': 0.03203841040213323, 'acc_norm': 0.2835820895522388, 'acc_norm_stderr': 0.03187187537919799}, 'hendrycksTest-prehistory': {'acc': 0.3271604938271605, 'acc_stderr': 0.026105673861409807, 'acc_norm': 0.27469135802469136, 'acc_norm_stderr': 0.024836057868294677}, 'hendrycksTest-professional_law': {'acc': 0.27640156453715775, 'acc_stderr': 0.011422153194553572, 'acc_norm': 0.29335071707953064, 'acc_norm_stderr': 0.011628520449582075}, 'hendrycksTest-virology': {'acc': 0.3855421686746988, 'acc_stderr': 0.03789134424611548, 'acc_norm': 0.30120481927710846, 'acc_norm_stderr': 0.035716092300534796}, 'hendrycksTest-machine_learning': {'acc': 0.21428571428571427, 'acc_stderr': 0.038946411200447915, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04109974682633932}, 'hendrycksTest-marketing': {'acc': 0.48717948717948717, 'acc_stderr': 0.032745319388423504, 'acc_norm': 0.43162393162393164, 'acc_norm_stderr': 0.0324483553531149}, 'hendrycksTest-us_foreign_policy': {'acc': 0.51, 'acc_stderr': 0.05024183937956912, 'acc_norm': 0.42, 'acc_norm_stderr': 0.049604496374885836}, 'hendrycksTest-high_school_european_history': {'acc': 0.3878787878787879, 'acc_stderr': 0.038049136539710114, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.0368105086916155}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.29743589743589743, 'acc_stderr': 0.023177408131465932, 'acc_norm': 0.2948717948717949, 'acc_norm_stderr': 0.023119362758232294}, 'hendrycksTest-logical_fallacies': {'acc': 0.25153374233128833, 'acc_stderr': 0.034089978868575295, 'acc_norm': 0.3006134969325153, 'acc_norm_stderr': 0.03602511318806771}, 'hendrycksTest-abstract_algebra': {'acc': 0.26, 'acc_stderr': 0.044084400227680794, 'acc_norm': 0.29, 'acc_norm_stderr': 0.04560480215720683}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.37823834196891193, 'acc_stderr': 0.03499807276193338, 'acc_norm': 0.29533678756476683, 'acc_norm_stderr': 0.03292296639155141}, 'hendrycksTest-moral_disputes': {'acc': 0.28034682080924855, 'acc_stderr': 0.02418242749657762, 'acc_norm': 0.315028901734104, 'acc_norm_stderr': 0.025009313790069706}, 'hendrycksTest-high_school_world_history': {'acc': 0.29957805907172996, 'acc_stderr': 0.0298180247497531, 'acc_norm': 0.31645569620253167, 'acc_norm_stderr': 0.030274974880218977}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.31092436974789917, 'acc_stderr': 0.030066761582977927, 'acc_norm': 0.38235294117647056, 'acc_norm_stderr': 0.03156663099215416}, 'hendrycksTest-college_medicine': {'acc': 0.2774566473988439, 'acc_stderr': 0.034140140070440354, 'acc_norm': 0.26011560693641617, 'acc_norm_stderr': 0.033450369167889925}, 'hendrycksTest-security_studies': {'acc': 0.43673469387755104, 'acc_stderr': 0.03175195237583323, 'acc_norm': 0.3510204081632653, 'acc_norm_stderr': 0.03055531675557364}, 'hendrycksTest-high_school_geography': {'acc': 0.3434343434343434, 'acc_stderr': 0.03383201223244442, 'acc_norm': 0.3282828282828283, 'acc_norm_stderr': 0.03345678422756776}, 'hendrycksTest-world_religions': {'acc': 0.5087719298245614, 'acc_stderr': 0.038342347441649924, 'acc_norm': 0.47953216374269003, 'acc_norm_stderr': 0.0383161053282193}, 'hendrycksTest-business_ethics': {'acc': 0.38, 'acc_stderr': 0.048783173121456344, 'acc_norm': 0.35, 'acc_norm_stderr': 0.04793724854411018}, 'hendrycksTest-professional_accounting': {'acc': 0.29432624113475175, 'acc_stderr': 0.02718712701150379, 'acc_norm': 0.30141843971631205, 'acc_norm_stderr': 0.02737412888263115}, 'hendrycksTest-global_facts': {'acc': 0.16, 'acc_stderr': 0.0368452949177471, 'acc_norm': 0.12, 'acc_norm_stderr': 0.032659863237109066}, 'hendrycksTest-high_school_us_history': {'acc': 0.35294117647058826, 'acc_stderr': 0.033540924375915195, 'acc_norm': 0.3137254901960784, 'acc_norm_stderr': 0.032566854844603886}, 'hendrycksTest-professional_psychology': {'acc': 0.3055555555555556, 'acc_stderr': 0.018635594034423976, 'acc_norm': 0.2875816993464052, 'acc_norm_stderr': 0.018311653053648222}, 'hendrycksTest-college_computer_science': {'acc': 0.35, 'acc_stderr': 0.0479372485441102, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04351941398892446}, 'hendrycksTest-philosophy': {'acc': 0.29260450160771706, 'acc_stderr': 0.02583989833487798, 'acc_norm': 0.3183279742765273, 'acc_norm_stderr': 0.026457225067811025}, 'hendrycksTest-anatomy': {'acc': 0.2962962962962963, 'acc_stderr': 0.03944624162501116, 'acc_norm': 0.24444444444444444, 'acc_norm_stderr': 0.037125378336148665}, 'hendrycksTest-nutrition': {'acc': 0.3758169934640523, 'acc_stderr': 0.027732834353363947, 'acc_norm': 0.4215686274509804, 'acc_norm_stderr': 0.02827549015679143}, 'hendrycksTest-computer_security': {'acc': 0.38, 'acc_stderr': 0.04878317312145632, 'acc_norm': 0.37, 'acc_norm_stderr': 0.048523658709391}, 'hendrycksTest-econometrics': {'acc': 0.2894736842105263, 'acc_stderr': 0.04266339443159392, 'acc_norm': 0.2719298245614035, 'acc_norm_stderr': 0.04185774424022056}, 'hendrycksTest-clinical_knowledge': {'acc': 0.33584905660377357, 'acc_stderr': 0.029067220146644826, 'acc_norm': 0.3584905660377358, 'acc_norm_stderr': 0.02951470358398176}, 'hendrycksTest-high_school_mathematics': {'acc': 0.24444444444444444, 'acc_stderr': 0.02620276653465215, 'acc_norm': 0.337037037037037, 'acc_norm_stderr': 0.028820884666253255}, 'hendrycksTest-medical_genetics': {'acc': 0.25, 'acc_stderr': 0.04351941398892446, 'acc_norm': 0.34, 'acc_norm_stderr': 0.04760952285695235}, 'hendrycksTest-public_relations': {'acc': 0.3, 'acc_stderr': 0.04389311454644286, 'acc_norm': 0.21818181818181817, 'acc_norm_stderr': 0.039559328617958335}, 'hendrycksTest-miscellaneous': {'acc': 0.388250319284802, 'acc_stderr': 0.017427673295544347, 'acc_norm': 0.2937420178799489, 'acc_norm_stderr': 0.01628775938849167}, 'hendrycksTest-management': {'acc': 0.3592233009708738, 'acc_stderr': 0.04750458399041694, 'acc_norm': 0.3106796116504854, 'acc_norm_stderr': 0.04582124160161551}, 'hendrycksTest-professional_medicine': {'acc': 0.2610294117647059, 'acc_stderr': 0.026679252270103128, 'acc_norm': 0.2757352941176471, 'acc_norm_stderr': 0.027146271936625162}, 'hendrycksTest-high_school_computer_science': {'acc': 0.35, 'acc_stderr': 0.04793724854411019, 'acc_norm': 0.29, 'acc_norm_stderr': 0.04560480215720683}, 'hendrycksTest-high_school_psychology': {'acc': 0.3229357798165138, 'acc_stderr': 0.020048115923415325, 'acc_norm': 0.25504587155963304, 'acc_norm_stderr': 0.01868850085653584}, 'hendrycksTest-high_school_chemistry': {'acc': 0.24630541871921183, 'acc_stderr': 0.030315099285617722, 'acc_norm': 0.3497536945812808, 'acc_norm_stderr': 0.03355400904969566}, 'hendrycksTest-college_mathematics': {'acc': 0.21, 'acc_stderr': 0.040936018074033256, 'acc_norm': 0.27, 'acc_norm_stderr': 0.044619604333847394}, 'hendrycksTest-high_school_statistics': {'acc': 0.3101851851851852, 'acc_stderr': 0.03154696285656628, 'acc_norm': 0.3101851851851852, 'acc_norm_stderr': 0.03154696285656629}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model': <models.LMClass.LMClass object at 0x7f92d7519250>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 14:59:20 root] (main.py 182): INFO Average accuracy 0.2860 - STEM
[2025-03-27 14:59:20 root] (main.py 182): INFO Average accuracy 0.3261 - humanities
[2025-03-27 14:59:20 root] (main.py 182): INFO Average accuracy 0.3541 - social sciences
[2025-03-27 14:59:20 root] (main.py 182): INFO Average accuracy 0.3270 - other (business, health, misc.)
[2025-03-27 14:59:20 root] (main.py 184): INFO Average accuracy: 0.3196
