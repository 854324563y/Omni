[2025-03-27 12:37:37 root] (main.py 258): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log/MMLU/llama-7b-w4a7', save_dir=None, resume='./log/llama-7b-w4a7/omni_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, aug_loss=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-03-27 12:37:44 root] (main.py 324): INFO === start quantization ===
[2025-03-27 12:37:44 root] (main.py 330): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-27 12:37:44 root] (omniquant.py 50): INFO Starting ...
[2025-03-27 12:37:46 root] (omniquant.py 193): INFO === Start quantize layer 0 ===
[2025-03-27 12:37:46 root] (omniquant.py 193): INFO === Start quantize layer 1 ===
[2025-03-27 12:37:47 root] (omniquant.py 193): INFO === Start quantize layer 2 ===
[2025-03-27 12:37:47 root] (omniquant.py 193): INFO === Start quantize layer 3 ===
[2025-03-27 12:37:48 root] (omniquant.py 193): INFO === Start quantize layer 4 ===
[2025-03-27 12:37:48 root] (omniquant.py 193): INFO === Start quantize layer 5 ===
[2025-03-27 12:37:48 root] (omniquant.py 193): INFO === Start quantize layer 6 ===
[2025-03-27 12:37:49 root] (omniquant.py 193): INFO === Start quantize layer 7 ===
[2025-03-27 12:37:49 root] (omniquant.py 193): INFO === Start quantize layer 8 ===
[2025-03-27 12:37:49 root] (omniquant.py 193): INFO === Start quantize layer 9 ===
[2025-03-27 12:37:50 root] (omniquant.py 193): INFO === Start quantize layer 10 ===
[2025-03-27 12:37:50 root] (omniquant.py 193): INFO === Start quantize layer 11 ===
[2025-03-27 12:37:50 root] (omniquant.py 193): INFO === Start quantize layer 12 ===
[2025-03-27 12:37:51 root] (omniquant.py 193): INFO === Start quantize layer 13 ===
[2025-03-27 12:37:51 root] (omniquant.py 193): INFO === Start quantize layer 14 ===
[2025-03-27 12:37:51 root] (omniquant.py 193): INFO === Start quantize layer 15 ===
[2025-03-27 12:37:52 root] (omniquant.py 193): INFO === Start quantize layer 16 ===
[2025-03-27 12:37:52 root] (omniquant.py 193): INFO === Start quantize layer 17 ===
[2025-03-27 12:37:53 root] (omniquant.py 193): INFO === Start quantize layer 18 ===
[2025-03-27 12:37:53 root] (omniquant.py 193): INFO === Start quantize layer 19 ===
[2025-03-27 12:37:53 root] (omniquant.py 193): INFO === Start quantize layer 20 ===
[2025-03-27 12:37:54 root] (omniquant.py 193): INFO === Start quantize layer 21 ===
[2025-03-27 12:37:54 root] (omniquant.py 193): INFO === Start quantize layer 22 ===
[2025-03-27 12:37:54 root] (omniquant.py 193): INFO === Start quantize layer 23 ===
[2025-03-27 12:37:55 root] (omniquant.py 193): INFO === Start quantize layer 24 ===
[2025-03-27 12:37:55 root] (omniquant.py 193): INFO === Start quantize layer 25 ===
[2025-03-27 12:37:55 root] (omniquant.py 193): INFO === Start quantize layer 26 ===
[2025-03-27 12:37:56 root] (omniquant.py 193): INFO === Start quantize layer 27 ===
[2025-03-27 12:37:56 root] (omniquant.py 193): INFO === Start quantize layer 28 ===
[2025-03-27 12:37:56 root] (omniquant.py 193): INFO === Start quantize layer 29 ===
[2025-03-27 12:37:57 root] (omniquant.py 193): INFO === Start quantize layer 30 ===
[2025-03-27 12:37:57 root] (omniquant.py 193): INFO === Start quantize layer 31 ===
[2025-03-27 12:37:58 root] (main.py 353): INFO 13.291511058807373
[2025-03-27 14:23:08 root] (main.py 155): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.30578512396694213, 'acc_stderr': 0.04205953933884123, 'acc_norm': 0.5454545454545454, 'acc_norm_stderr': 0.04545454545454548}, 'hendrycksTest-high_school_biology': {'acc': 0.25161290322580643, 'acc_stderr': 0.024685979286239956, 'acc_norm': 0.29354838709677417, 'acc_norm_stderr': 0.02590608702131929}, 'hendrycksTest-college_physics': {'acc': 0.30392156862745096, 'acc_stderr': 0.04576665403207763, 'acc_norm': 0.30392156862745096, 'acc_norm_stderr': 0.045766654032077636}, 'hendrycksTest-college_biology': {'acc': 0.2708333333333333, 'acc_stderr': 0.037161774375660164, 'acc_norm': 0.2708333333333333, 'acc_norm_stderr': 0.03716177437566017}, 'hendrycksTest-electrical_engineering': {'acc': 0.32413793103448274, 'acc_stderr': 0.03900432069185555, 'acc_norm': 0.3310344827586207, 'acc_norm_stderr': 0.039215453124671215}, 'hendrycksTest-high_school_physics': {'acc': 0.25165562913907286, 'acc_stderr': 0.035433042343899844, 'acc_norm': 0.25165562913907286, 'acc_norm_stderr': 0.035433042343899844}, 'hendrycksTest-elementary_mathematics': {'acc': 0.25396825396825395, 'acc_stderr': 0.022418042891113935, 'acc_norm': 0.26455026455026454, 'acc_norm_stderr': 0.022717467897708638}, 'hendrycksTest-formal_logic': {'acc': 0.2857142857142857, 'acc_stderr': 0.04040610178208841, 'acc_norm': 0.30158730158730157, 'acc_norm_stderr': 0.04104947269903394}, 'hendrycksTest-human_aging': {'acc': 0.2914798206278027, 'acc_stderr': 0.030500283176545906, 'acc_norm': 0.23766816143497757, 'acc_norm_stderr': 0.028568079464714274}, 'hendrycksTest-jurisprudence': {'acc': 0.37037037037037035, 'acc_stderr': 0.04668408033024932, 'acc_norm': 0.4166666666666667, 'acc_norm_stderr': 0.04766075165356461}, 'hendrycksTest-moral_scenarios': {'acc': 0.24022346368715083, 'acc_stderr': 0.014288343803925295, 'acc_norm': 0.2581005586592179, 'acc_norm_stderr': 0.014635185616527836}, 'hendrycksTest-astronomy': {'acc': 0.29605263157894735, 'acc_stderr': 0.03715062154998905, 'acc_norm': 0.39473684210526316, 'acc_norm_stderr': 0.039777499346220734}, 'hendrycksTest-conceptual_physics': {'acc': 0.2765957446808511, 'acc_stderr': 0.029241883869628806, 'acc_norm': 0.2170212765957447, 'acc_norm_stderr': 0.026947483121496224}, 'hendrycksTest-college_chemistry': {'acc': 0.24, 'acc_stderr': 0.04292346959909283, 'acc_norm': 0.33, 'acc_norm_stderr': 0.04725815626252604}, 'hendrycksTest-human_sexuality': {'acc': 0.37404580152671757, 'acc_stderr': 0.04243869242230524, 'acc_norm': 0.29770992366412213, 'acc_norm_stderr': 0.040103589424622034}, 'hendrycksTest-sociology': {'acc': 0.27860696517412936, 'acc_stderr': 0.031700561834973086, 'acc_norm': 0.29850746268656714, 'acc_norm_stderr': 0.032357437893550424}, 'hendrycksTest-prehistory': {'acc': 0.27469135802469136, 'acc_stderr': 0.02483605786829468, 'acc_norm': 0.23765432098765432, 'acc_norm_stderr': 0.023683591837008564}, 'hendrycksTest-professional_law': {'acc': 0.2633637548891786, 'acc_stderr': 0.011249506403605291, 'acc_norm': 0.2920469361147327, 'acc_norm_stderr': 0.01161334913627182}, 'hendrycksTest-virology': {'acc': 0.3373493975903614, 'acc_stderr': 0.0368078369072758, 'acc_norm': 0.3253012048192771, 'acc_norm_stderr': 0.03647168523683228}, 'hendrycksTest-machine_learning': {'acc': 0.32142857142857145, 'acc_stderr': 0.044328040552915206, 'acc_norm': 0.24107142857142858, 'acc_norm_stderr': 0.04059867246952687}, 'hendrycksTest-marketing': {'acc': 0.4358974358974359, 'acc_stderr': 0.032485775115783995, 'acc_norm': 0.4017094017094017, 'acc_norm_stderr': 0.03211693751051621}, 'hendrycksTest-us_foreign_policy': {'acc': 0.45, 'acc_stderr': 0.05, 'acc_norm': 0.39, 'acc_norm_stderr': 0.04902071300001975}, 'hendrycksTest-high_school_european_history': {'acc': 0.3212121212121212, 'acc_stderr': 0.03646204963253812, 'acc_norm': 0.28484848484848485, 'acc_norm_stderr': 0.035243908445117836}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.3076923076923077, 'acc_stderr': 0.023400928918310512, 'acc_norm': 0.3076923076923077, 'acc_norm_stderr': 0.023400928918310512}, 'hendrycksTest-logical_fallacies': {'acc': 0.2392638036809816, 'acc_stderr': 0.033519538795212696, 'acc_norm': 0.3312883435582822, 'acc_norm_stderr': 0.03697983910025588}, 'hendrycksTest-abstract_algebra': {'acc': 0.31, 'acc_stderr': 0.04648231987117316, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04351941398892446}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.34196891191709844, 'acc_stderr': 0.03423465100104282, 'acc_norm': 0.3160621761658031, 'acc_norm_stderr': 0.033553973696861736}, 'hendrycksTest-moral_disputes': {'acc': 0.2630057803468208, 'acc_stderr': 0.02370309952525817, 'acc_norm': 0.30057803468208094, 'acc_norm_stderr': 0.024685316867257803}, 'hendrycksTest-high_school_world_history': {'acc': 0.26582278481012656, 'acc_stderr': 0.02875679962965834, 'acc_norm': 0.28270042194092826, 'acc_norm_stderr': 0.029312814153955924}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.27310924369747897, 'acc_stderr': 0.028942004040998164, 'acc_norm': 0.35714285714285715, 'acc_norm_stderr': 0.031124619309328177}, 'hendrycksTest-college_medicine': {'acc': 0.2947976878612717, 'acc_stderr': 0.03476599607516479, 'acc_norm': 0.2658959537572254, 'acc_norm_stderr': 0.03368762932259429}, 'hendrycksTest-security_studies': {'acc': 0.4, 'acc_stderr': 0.031362502409358936, 'acc_norm': 0.2979591836734694, 'acc_norm_stderr': 0.02927956741106567}, 'hendrycksTest-high_school_geography': {'acc': 0.2828282828282828, 'acc_stderr': 0.032087795587867514, 'acc_norm': 0.31313131313131315, 'acc_norm_stderr': 0.033042050878136525}, 'hendrycksTest-world_religions': {'acc': 0.4269005847953216, 'acc_stderr': 0.03793620616529916, 'acc_norm': 0.45614035087719296, 'acc_norm_stderr': 0.038200425866029654}, 'hendrycksTest-business_ethics': {'acc': 0.41, 'acc_stderr': 0.04943110704237102, 'acc_norm': 0.36, 'acc_norm_stderr': 0.048241815132442176}, 'hendrycksTest-professional_accounting': {'acc': 0.2695035460992908, 'acc_stderr': 0.026469036818590627, 'acc_norm': 0.30851063829787234, 'acc_norm_stderr': 0.027553366165101366}, 'hendrycksTest-global_facts': {'acc': 0.27, 'acc_stderr': 0.044619604333847394, 'acc_norm': 0.26, 'acc_norm_stderr': 0.04408440022768077}, 'hendrycksTest-high_school_us_history': {'acc': 0.3088235294117647, 'acc_stderr': 0.03242661719827218, 'acc_norm': 0.27941176470588236, 'acc_norm_stderr': 0.03149328104507957}, 'hendrycksTest-professional_psychology': {'acc': 0.26633986928104575, 'acc_stderr': 0.01788318813466719, 'acc_norm': 0.2581699346405229, 'acc_norm_stderr': 0.01770453165325007}, 'hendrycksTest-college_computer_science': {'acc': 0.31, 'acc_stderr': 0.04648231987117316, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117316}, 'hendrycksTest-philosophy': {'acc': 0.26366559485530544, 'acc_stderr': 0.02502553850053234, 'acc_norm': 0.2990353697749196, 'acc_norm_stderr': 0.02600330111788513}, 'hendrycksTest-anatomy': {'acc': 0.3037037037037037, 'acc_stderr': 0.039725528847851375, 'acc_norm': 0.22962962962962963, 'acc_norm_stderr': 0.03633384414073464}, 'hendrycksTest-nutrition': {'acc': 0.3464052287581699, 'acc_stderr': 0.027245613047215355, 'acc_norm': 0.39869281045751637, 'acc_norm_stderr': 0.028036092273891762}, 'hendrycksTest-computer_security': {'acc': 0.29, 'acc_stderr': 0.04560480215720683, 'acc_norm': 0.38, 'acc_norm_stderr': 0.04878317312145632}, 'hendrycksTest-econometrics': {'acc': 0.2894736842105263, 'acc_stderr': 0.04266339443159393, 'acc_norm': 0.18421052631578946, 'acc_norm_stderr': 0.03646758875075566}, 'hendrycksTest-clinical_knowledge': {'acc': 0.3132075471698113, 'acc_stderr': 0.028544793319055326, 'acc_norm': 0.32452830188679244, 'acc_norm_stderr': 0.028815615713432108}, 'hendrycksTest-high_school_mathematics': {'acc': 0.24444444444444444, 'acc_stderr': 0.02620276653465215, 'acc_norm': 0.3037037037037037, 'acc_norm_stderr': 0.028037929969114986}, 'hendrycksTest-medical_genetics': {'acc': 0.29, 'acc_stderr': 0.045604802157206845, 'acc_norm': 0.36, 'acc_norm_stderr': 0.04824181513244218}, 'hendrycksTest-public_relations': {'acc': 0.3, 'acc_stderr': 0.04389311454644286, 'acc_norm': 0.18181818181818182, 'acc_norm_stderr': 0.036942843353378}, 'hendrycksTest-miscellaneous': {'acc': 0.3652618135376756, 'acc_stderr': 0.017218530028838643, 'acc_norm': 0.30779054916985954, 'acc_norm_stderr': 0.01650604504515563}, 'hendrycksTest-management': {'acc': 0.3106796116504854, 'acc_stderr': 0.04582124160161551, 'acc_norm': 0.32038834951456313, 'acc_norm_stderr': 0.04620284082280039}, 'hendrycksTest-professional_medicine': {'acc': 0.2610294117647059, 'acc_stderr': 0.026679252270103128, 'acc_norm': 0.2757352941176471, 'acc_norm_stderr': 0.027146271936625162}, 'hendrycksTest-high_school_computer_science': {'acc': 0.29, 'acc_stderr': 0.04560480215720683, 'acc_norm': 0.35, 'acc_norm_stderr': 0.04793724854411022}, 'hendrycksTest-high_school_psychology': {'acc': 0.30091743119266057, 'acc_stderr': 0.019664751366802114, 'acc_norm': 0.24770642201834864, 'acc_norm_stderr': 0.01850814360254781}, 'hendrycksTest-high_school_chemistry': {'acc': 0.21182266009852216, 'acc_stderr': 0.02874898368994106, 'acc_norm': 0.2660098522167488, 'acc_norm_stderr': 0.03108982600293752}, 'hendrycksTest-college_mathematics': {'acc': 0.17, 'acc_stderr': 0.03775251680686371, 'acc_norm': 0.26, 'acc_norm_stderr': 0.0440844002276808}, 'hendrycksTest-high_school_statistics': {'acc': 0.3055555555555556, 'acc_stderr': 0.03141554629402545, 'acc_norm': 0.30092592592592593, 'acc_norm_stderr': 0.03128039084329881}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model': <models.LMClass.LMClass object at 0x7fe88ddae990>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 14:23:08 root] (main.py 182): INFO Average accuracy 0.2734 - STEM
[2025-03-27 14:23:08 root] (main.py 182): INFO Average accuracy 0.2945 - humanities
[2025-03-27 14:23:08 root] (main.py 182): INFO Average accuracy 0.3221 - social sciences
[2025-03-27 14:23:08 root] (main.py 182): INFO Average accuracy 0.3214 - other (business, health, misc.)
[2025-03-27 14:23:08 root] (main.py 184): INFO Average accuracy: 0.3003
