[2025-03-27 12:45:26 root] (main.py 258): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log/MMLU/Llama-2-7b-w4a7', save_dir=None, resume='./log/Llama-2-7b-w4a7/omni_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, aug_loss=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-03-27 12:49:09 root] (main.py 324): INFO === start quantization ===
[2025-03-27 12:49:09 root] (main.py 330): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-27 12:49:10 root] (omniquant.py 50): INFO Starting ...
[2025-03-27 12:49:12 root] (omniquant.py 193): INFO === Start quantize layer 0 ===
[2025-03-27 12:49:13 root] (omniquant.py 193): INFO === Start quantize layer 1 ===
[2025-03-27 12:49:13 root] (omniquant.py 193): INFO === Start quantize layer 2 ===
[2025-03-27 12:49:14 root] (omniquant.py 193): INFO === Start quantize layer 3 ===
[2025-03-27 12:49:14 root] (omniquant.py 193): INFO === Start quantize layer 4 ===
[2025-03-27 12:49:14 root] (omniquant.py 193): INFO === Start quantize layer 5 ===
[2025-03-27 12:49:15 root] (omniquant.py 193): INFO === Start quantize layer 6 ===
[2025-03-27 12:49:15 root] (omniquant.py 193): INFO === Start quantize layer 7 ===
[2025-03-27 12:49:15 root] (omniquant.py 193): INFO === Start quantize layer 8 ===
[2025-03-27 12:49:16 root] (omniquant.py 193): INFO === Start quantize layer 9 ===
[2025-03-27 12:49:16 root] (omniquant.py 193): INFO === Start quantize layer 10 ===
[2025-03-27 12:49:17 root] (omniquant.py 193): INFO === Start quantize layer 11 ===
[2025-03-27 12:49:17 root] (omniquant.py 193): INFO === Start quantize layer 12 ===
[2025-03-27 12:49:17 root] (omniquant.py 193): INFO === Start quantize layer 13 ===
[2025-03-27 12:49:18 root] (omniquant.py 193): INFO === Start quantize layer 14 ===
[2025-03-27 12:49:18 root] (omniquant.py 193): INFO === Start quantize layer 15 ===
[2025-03-27 12:49:18 root] (omniquant.py 193): INFO === Start quantize layer 16 ===
[2025-03-27 12:49:19 root] (omniquant.py 193): INFO === Start quantize layer 17 ===
[2025-03-27 12:49:19 root] (omniquant.py 193): INFO === Start quantize layer 18 ===
[2025-03-27 12:49:19 root] (omniquant.py 193): INFO === Start quantize layer 19 ===
[2025-03-27 12:49:20 root] (omniquant.py 193): INFO === Start quantize layer 20 ===
[2025-03-27 12:49:20 root] (omniquant.py 193): INFO === Start quantize layer 21 ===
[2025-03-27 12:49:20 root] (omniquant.py 193): INFO === Start quantize layer 22 ===
[2025-03-27 12:49:21 root] (omniquant.py 193): INFO === Start quantize layer 23 ===
[2025-03-27 12:49:21 root] (omniquant.py 193): INFO === Start quantize layer 24 ===
[2025-03-27 12:49:21 root] (omniquant.py 193): INFO === Start quantize layer 25 ===
[2025-03-27 12:49:22 root] (omniquant.py 193): INFO === Start quantize layer 26 ===
[2025-03-27 12:49:22 root] (omniquant.py 193): INFO === Start quantize layer 27 ===
[2025-03-27 12:49:22 root] (omniquant.py 193): INFO === Start quantize layer 28 ===
[2025-03-27 12:49:23 root] (omniquant.py 193): INFO === Start quantize layer 29 ===
[2025-03-27 12:49:23 root] (omniquant.py 193): INFO === Start quantize layer 30 ===
[2025-03-27 12:49:24 root] (omniquant.py 193): INFO === Start quantize layer 31 ===
[2025-03-27 12:49:24 root] (main.py 353): INFO 14.915830612182617
[2025-03-27 14:37:01 root] (main.py 155): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.32231404958677684, 'acc_stderr': 0.04266416363352167, 'acc_norm': 0.5206611570247934, 'acc_norm_stderr': 0.04560456086387235}, 'hendrycksTest-high_school_biology': {'acc': 0.2903225806451613, 'acc_stderr': 0.02582210611941589, 'acc_norm': 0.2870967741935484, 'acc_norm_stderr': 0.025736542745594525}, 'hendrycksTest-college_physics': {'acc': 0.23529411764705882, 'acc_stderr': 0.04220773659171452, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.04690650298201942}, 'hendrycksTest-college_biology': {'acc': 0.2569444444444444, 'acc_stderr': 0.03653946969442099, 'acc_norm': 0.24305555555555555, 'acc_norm_stderr': 0.03586879280080341}, 'hendrycksTest-electrical_engineering': {'acc': 0.3103448275862069, 'acc_stderr': 0.038552896163789485, 'acc_norm': 0.31724137931034485, 'acc_norm_stderr': 0.03878352372138622}, 'hendrycksTest-high_school_physics': {'acc': 0.2847682119205298, 'acc_stderr': 0.03684881521389023, 'acc_norm': 0.25165562913907286, 'acc_norm_stderr': 0.03543304234389985}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2962962962962963, 'acc_stderr': 0.023517294335963286, 'acc_norm': 0.2857142857142857, 'acc_norm_stderr': 0.023266512213730554}, 'hendrycksTest-formal_logic': {'acc': 0.3492063492063492, 'acc_stderr': 0.042639068927951315, 'acc_norm': 0.30952380952380953, 'acc_norm_stderr': 0.041349130183033156}, 'hendrycksTest-human_aging': {'acc': 0.29596412556053814, 'acc_stderr': 0.030636591348699803, 'acc_norm': 0.2242152466367713, 'acc_norm_stderr': 0.027991534258519524}, 'hendrycksTest-jurisprudence': {'acc': 0.2777777777777778, 'acc_stderr': 0.043300437496507437, 'acc_norm': 0.42592592592592593, 'acc_norm_stderr': 0.0478034362693679}, 'hendrycksTest-moral_scenarios': {'acc': 0.25251396648044694, 'acc_stderr': 0.01453033020146862, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.3355263157894737, 'acc_stderr': 0.03842498559395268, 'acc_norm': 0.3815789473684211, 'acc_norm_stderr': 0.039531733777491945}, 'hendrycksTest-conceptual_physics': {'acc': 0.23829787234042554, 'acc_stderr': 0.02785125297388977, 'acc_norm': 0.2127659574468085, 'acc_norm_stderr': 0.026754391348039763}, 'hendrycksTest-college_chemistry': {'acc': 0.27, 'acc_stderr': 0.044619604333847394, 'acc_norm': 0.34, 'acc_norm_stderr': 0.04760952285695235}, 'hendrycksTest-human_sexuality': {'acc': 0.4351145038167939, 'acc_stderr': 0.043482080516448585, 'acc_norm': 0.2900763358778626, 'acc_norm_stderr': 0.03980066246467766}, 'hendrycksTest-sociology': {'acc': 0.2885572139303483, 'acc_stderr': 0.03203841040213322, 'acc_norm': 0.31840796019900497, 'acc_norm_stderr': 0.03294118479054095}, 'hendrycksTest-prehistory': {'acc': 0.3148148148148148, 'acc_stderr': 0.025842248700902175, 'acc_norm': 0.25, 'acc_norm_stderr': 0.02409347123262133}, 'hendrycksTest-professional_law': {'acc': 0.25684485006518903, 'acc_stderr': 0.011158455853098848, 'acc_norm': 0.27640156453715775, 'acc_norm_stderr': 0.01142215319455358}, 'hendrycksTest-virology': {'acc': 0.27710843373493976, 'acc_stderr': 0.03484331592680586, 'acc_norm': 0.2710843373493976, 'acc_norm_stderr': 0.034605799075530276}, 'hendrycksTest-machine_learning': {'acc': 0.2767857142857143, 'acc_stderr': 0.042466243366976256, 'acc_norm': 0.23214285714285715, 'acc_norm_stderr': 0.04007341809755804}, 'hendrycksTest-marketing': {'acc': 0.39316239316239315, 'acc_stderr': 0.03199957924651048, 'acc_norm': 0.3547008547008547, 'acc_norm_stderr': 0.031342504862454025}, 'hendrycksTest-us_foreign_policy': {'acc': 0.41, 'acc_stderr': 0.049431107042371025, 'acc_norm': 0.39, 'acc_norm_stderr': 0.04902071300001975}, 'hendrycksTest-high_school_european_history': {'acc': 0.296969696969697, 'acc_stderr': 0.035679697722680474, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.0368105086916155}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.32564102564102565, 'acc_stderr': 0.02375966576741229, 'acc_norm': 0.30256410256410254, 'acc_norm_stderr': 0.02329088805377273}, 'hendrycksTest-logical_fallacies': {'acc': 0.25153374233128833, 'acc_stderr': 0.034089978868575295, 'acc_norm': 0.3374233128834356, 'acc_norm_stderr': 0.03714908409935574}, 'hendrycksTest-abstract_algebra': {'acc': 0.29, 'acc_stderr': 0.04560480215720683, 'acc_norm': 0.28, 'acc_norm_stderr': 0.04512608598542127}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.34196891191709844, 'acc_stderr': 0.03423465100104282, 'acc_norm': 0.31088082901554404, 'acc_norm_stderr': 0.03340361906276588}, 'hendrycksTest-moral_disputes': {'acc': 0.2947976878612717, 'acc_stderr': 0.024547617794803835, 'acc_norm': 0.33236994219653176, 'acc_norm_stderr': 0.025361168749688218}, 'hendrycksTest-high_school_world_history': {'acc': 0.35864978902953587, 'acc_stderr': 0.03121956944530185, 'acc_norm': 0.32489451476793246, 'acc_norm_stderr': 0.030486039389105296}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.31512605042016806, 'acc_stderr': 0.03017680828897434, 'acc_norm': 0.37815126050420167, 'acc_norm_stderr': 0.03149930577784906}, 'hendrycksTest-college_medicine': {'acc': 0.24277456647398843, 'acc_stderr': 0.0326926380614177, 'acc_norm': 0.2774566473988439, 'acc_norm_stderr': 0.034140140070440354}, 'hendrycksTest-security_studies': {'acc': 0.46530612244897956, 'acc_stderr': 0.03193207024425314, 'acc_norm': 0.34285714285714286, 'acc_norm_stderr': 0.030387262919547728}, 'hendrycksTest-high_school_geography': {'acc': 0.26262626262626265, 'acc_stderr': 0.031353050095330855, 'acc_norm': 0.29292929292929293, 'acc_norm_stderr': 0.03242497958178815}, 'hendrycksTest-world_religions': {'acc': 0.3391812865497076, 'acc_stderr': 0.036310534964889056, 'acc_norm': 0.36257309941520466, 'acc_norm_stderr': 0.036871306155620606}, 'hendrycksTest-business_ethics': {'acc': 0.35, 'acc_stderr': 0.04793724854411019, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-professional_accounting': {'acc': 0.25886524822695034, 'acc_stderr': 0.026129572527180848, 'acc_norm': 0.2695035460992908, 'acc_norm_stderr': 0.026469036818590634}, 'hendrycksTest-global_facts': {'acc': 0.24, 'acc_stderr': 0.042923469599092816, 'acc_norm': 0.24, 'acc_norm_stderr': 0.04292346959909282}, 'hendrycksTest-high_school_us_history': {'acc': 0.3235294117647059, 'acc_stderr': 0.03283472056108566, 'acc_norm': 0.3137254901960784, 'acc_norm_stderr': 0.03256685484460388}, 'hendrycksTest-professional_psychology': {'acc': 0.2875816993464052, 'acc_stderr': 0.018311653053648222, 'acc_norm': 0.26633986928104575, 'acc_norm_stderr': 0.017883188134667192}, 'hendrycksTest-college_computer_science': {'acc': 0.31, 'acc_stderr': 0.04648231987117316, 'acc_norm': 0.26, 'acc_norm_stderr': 0.0440844002276808}, 'hendrycksTest-philosophy': {'acc': 0.3086816720257235, 'acc_stderr': 0.026236965881153266, 'acc_norm': 0.31511254019292606, 'acc_norm_stderr': 0.026385273703464482}, 'hendrycksTest-anatomy': {'acc': 0.31851851851851853, 'acc_stderr': 0.04024778401977112, 'acc_norm': 0.22962962962962963, 'acc_norm_stderr': 0.03633384414073463}, 'hendrycksTest-nutrition': {'acc': 0.33986928104575165, 'acc_stderr': 0.02712195607138886, 'acc_norm': 0.4117647058823529, 'acc_norm_stderr': 0.02818059632825929}, 'hendrycksTest-computer_security': {'acc': 0.26, 'acc_stderr': 0.044084400227680794, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117316}, 'hendrycksTest-econometrics': {'acc': 0.2631578947368421, 'acc_stderr': 0.041424397194893624, 'acc_norm': 0.21052631578947367, 'acc_norm_stderr': 0.038351539543994194}, 'hendrycksTest-clinical_knowledge': {'acc': 0.29056603773584905, 'acc_stderr': 0.027943219989337135, 'acc_norm': 0.3660377358490566, 'acc_norm_stderr': 0.029647813539365245}, 'hendrycksTest-high_school_mathematics': {'acc': 0.1962962962962963, 'acc_stderr': 0.024217421327417138, 'acc_norm': 0.23703703703703705, 'acc_norm_stderr': 0.025928876132766114}, 'hendrycksTest-medical_genetics': {'acc': 0.21, 'acc_stderr': 0.04093601807403326, 'acc_norm': 0.36, 'acc_norm_stderr': 0.04824181513244218}, 'hendrycksTest-public_relations': {'acc': 0.2727272727272727, 'acc_stderr': 0.04265792110940589, 'acc_norm': 0.14545454545454545, 'acc_norm_stderr': 0.033768983198330826}, 'hendrycksTest-miscellaneous': {'acc': 0.3333333333333333, 'acc_stderr': 0.016857391247472552, 'acc_norm': 0.2771392081736909, 'acc_norm_stderr': 0.016005636294122425}, 'hendrycksTest-management': {'acc': 0.3106796116504854, 'acc_stderr': 0.04582124160161552, 'acc_norm': 0.3106796116504854, 'acc_norm_stderr': 0.04582124160161552}, 'hendrycksTest-professional_medicine': {'acc': 0.2977941176470588, 'acc_stderr': 0.027778298701545443, 'acc_norm': 0.2647058823529412, 'acc_norm_stderr': 0.026799562024887674}, 'hendrycksTest-high_school_computer_science': {'acc': 0.27, 'acc_stderr': 0.044619604333847394, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-high_school_psychology': {'acc': 0.28623853211009176, 'acc_stderr': 0.019379436628919958, 'acc_norm': 0.25321100917431194, 'acc_norm_stderr': 0.018644073041375046}, 'hendrycksTest-high_school_chemistry': {'acc': 0.27586206896551724, 'acc_stderr': 0.03144712581678245, 'acc_norm': 0.32019704433497537, 'acc_norm_stderr': 0.032826493853041504}, 'hendrycksTest-college_mathematics': {'acc': 0.17, 'acc_stderr': 0.0377525168068637, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-high_school_statistics': {'acc': 0.2962962962962963, 'acc_stderr': 0.03114144782353604, 'acc_norm': 0.3101851851851852, 'acc_norm_stderr': 0.031546962856566295}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model': <models.LMClass.LMClass object at 0x7f396cbb5210>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 14:37:01 root] (main.py 182): INFO Average accuracy 0.2702 - STEM
[2025-03-27 14:37:01 root] (main.py 182): INFO Average accuracy 0.3036 - humanities
[2025-03-27 14:37:01 root] (main.py 182): INFO Average accuracy 0.3295 - social sciences
[2025-03-27 14:37:01 root] (main.py 182): INFO Average accuracy 0.2970 - other (business, health, misc.)
[2025-03-27 14:37:01 root] (main.py 184): INFO Average accuracy: 0.2969
