[2025-03-27 12:46:57 root] (main.py 258): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log/MMLU/Llama-2-13b-w4a6', save_dir=None, resume='./log/Llama-2-13b-w4a6/omni_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, aug_loss=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-03-27 12:46:58 root] (main.py 324): INFO === start quantization ===
[2025-03-27 12:46:59 root] (main.py 330): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-27 12:46:59 root] (omniquant.py 50): INFO Starting ...
[2025-03-27 12:47:17 root] (omniquant.py 193): INFO === Start quantize layer 0 ===
[2025-03-27 12:47:17 root] (omniquant.py 193): INFO === Start quantize layer 1 ===
[2025-03-27 12:47:27 root] (omniquant.py 193): INFO === Start quantize layer 2 ===
[2025-03-27 12:47:36 root] (omniquant.py 193): INFO === Start quantize layer 3 ===
[2025-03-27 12:47:46 root] (omniquant.py 193): INFO === Start quantize layer 4 ===
[2025-03-27 12:47:56 root] (omniquant.py 193): INFO === Start quantize layer 5 ===
[2025-03-27 12:48:05 root] (omniquant.py 193): INFO === Start quantize layer 6 ===
[2025-03-27 12:48:15 root] (omniquant.py 193): INFO === Start quantize layer 7 ===
[2025-03-27 12:48:24 root] (omniquant.py 193): INFO === Start quantize layer 8 ===
[2025-03-27 12:48:34 root] (omniquant.py 193): INFO === Start quantize layer 9 ===
[2025-03-27 12:48:44 root] (omniquant.py 193): INFO === Start quantize layer 10 ===
[2025-03-27 12:48:53 root] (omniquant.py 193): INFO === Start quantize layer 11 ===
[2025-03-27 12:49:03 root] (omniquant.py 193): INFO === Start quantize layer 12 ===
[2025-03-27 12:49:11 root] (omniquant.py 193): INFO === Start quantize layer 13 ===
[2025-03-27 12:49:18 root] (omniquant.py 193): INFO === Start quantize layer 14 ===
[2025-03-27 12:49:25 root] (omniquant.py 193): INFO === Start quantize layer 15 ===
[2025-03-27 12:49:34 root] (omniquant.py 193): INFO === Start quantize layer 16 ===
[2025-03-27 12:49:43 root] (omniquant.py 193): INFO === Start quantize layer 17 ===
[2025-03-27 12:49:51 root] (omniquant.py 193): INFO === Start quantize layer 18 ===
[2025-03-27 12:49:59 root] (omniquant.py 193): INFO === Start quantize layer 19 ===
[2025-03-27 12:50:06 root] (omniquant.py 193): INFO === Start quantize layer 20 ===
[2025-03-27 12:50:14 root] (omniquant.py 193): INFO === Start quantize layer 21 ===
[2025-03-27 12:50:20 root] (omniquant.py 193): INFO === Start quantize layer 22 ===
[2025-03-27 12:50:27 root] (omniquant.py 193): INFO === Start quantize layer 23 ===
[2025-03-27 12:50:35 root] (omniquant.py 193): INFO === Start quantize layer 24 ===
[2025-03-27 12:50:44 root] (omniquant.py 193): INFO === Start quantize layer 25 ===
[2025-03-27 12:50:53 root] (omniquant.py 193): INFO === Start quantize layer 26 ===
[2025-03-27 12:51:02 root] (omniquant.py 193): INFO === Start quantize layer 27 ===
[2025-03-27 12:51:11 root] (omniquant.py 193): INFO === Start quantize layer 28 ===
[2025-03-27 12:51:19 root] (omniquant.py 193): INFO === Start quantize layer 29 ===
[2025-03-27 12:51:28 root] (omniquant.py 193): INFO === Start quantize layer 30 ===
[2025-03-27 12:51:36 root] (omniquant.py 193): INFO === Start quantize layer 31 ===
[2025-03-27 12:51:46 root] (omniquant.py 193): INFO === Start quantize layer 32 ===
[2025-03-27 12:51:55 root] (omniquant.py 193): INFO === Start quantize layer 33 ===
[2025-03-27 12:52:03 root] (omniquant.py 193): INFO === Start quantize layer 34 ===
[2025-03-27 12:52:10 root] (omniquant.py 193): INFO === Start quantize layer 35 ===
[2025-03-27 12:52:18 root] (omniquant.py 193): INFO === Start quantize layer 36 ===
[2025-03-27 12:52:26 root] (omniquant.py 193): INFO === Start quantize layer 37 ===
[2025-03-27 12:52:34 root] (omniquant.py 193): INFO === Start quantize layer 38 ===
[2025-03-27 12:52:42 root] (omniquant.py 193): INFO === Start quantize layer 39 ===
[2025-03-27 12:52:51 root] (main.py 353): INFO 352.35783648490906
[2025-03-27 15:05:10 root] (main.py 155): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.38016528925619836, 'acc_stderr': 0.04431324501968432, 'acc_norm': 0.5867768595041323, 'acc_norm_stderr': 0.04495087843548408}, 'hendrycksTest-high_school_biology': {'acc': 0.35161290322580646, 'acc_stderr': 0.027162537826948458, 'acc_norm': 0.3193548387096774, 'acc_norm_stderr': 0.026522709674667765}, 'hendrycksTest-college_physics': {'acc': 0.3235294117647059, 'acc_stderr': 0.04655010411319616, 'acc_norm': 0.3137254901960784, 'acc_norm_stderr': 0.04617034827006717}, 'hendrycksTest-college_biology': {'acc': 0.3402777777777778, 'acc_stderr': 0.03962135573486219, 'acc_norm': 0.2638888888888889, 'acc_norm_stderr': 0.03685651095897532}, 'hendrycksTest-electrical_engineering': {'acc': 0.36551724137931035, 'acc_stderr': 0.040131241954243856, 'acc_norm': 0.36551724137931035, 'acc_norm_stderr': 0.04013124195424386}, 'hendrycksTest-high_school_physics': {'acc': 0.2781456953642384, 'acc_stderr': 0.03658603262763743, 'acc_norm': 0.2847682119205298, 'acc_norm_stderr': 0.03684881521389023}, 'hendrycksTest-elementary_mathematics': {'acc': 0.3253968253968254, 'acc_stderr': 0.02413015829976262, 'acc_norm': 0.31216931216931215, 'acc_norm_stderr': 0.023865206836972595}, 'hendrycksTest-formal_logic': {'acc': 0.2777777777777778, 'acc_stderr': 0.04006168083848877, 'acc_norm': 0.29365079365079366, 'acc_norm_stderr': 0.04073524322147126}, 'hendrycksTest-human_aging': {'acc': 0.3094170403587444, 'acc_stderr': 0.031024411740572206, 'acc_norm': 0.22869955156950672, 'acc_norm_stderr': 0.02818824004692919}, 'hendrycksTest-jurisprudence': {'acc': 0.3611111111111111, 'acc_stderr': 0.04643454608906275, 'acc_norm': 0.4537037037037037, 'acc_norm_stderr': 0.04812917324536823}, 'hendrycksTest-moral_scenarios': {'acc': 0.2927374301675978, 'acc_stderr': 0.015218109544410201, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.4144736842105263, 'acc_stderr': 0.04008973785779206, 'acc_norm': 0.4605263157894737, 'acc_norm_stderr': 0.04056242252249033}, 'hendrycksTest-conceptual_physics': {'acc': 0.28936170212765955, 'acc_stderr': 0.02964400657700962, 'acc_norm': 0.22127659574468084, 'acc_norm_stderr': 0.02713634960242406}, 'hendrycksTest-college_chemistry': {'acc': 0.29, 'acc_stderr': 0.045604802157206845, 'acc_norm': 0.35, 'acc_norm_stderr': 0.0479372485441102}, 'hendrycksTest-human_sexuality': {'acc': 0.46564885496183206, 'acc_stderr': 0.043749285605997376, 'acc_norm': 0.31297709923664124, 'acc_norm_stderr': 0.04066962905677697}, 'hendrycksTest-sociology': {'acc': 0.39800995024875624, 'acc_stderr': 0.03461199429040013, 'acc_norm': 0.3582089552238806, 'acc_norm_stderr': 0.03390393042268815}, 'hendrycksTest-prehistory': {'acc': 0.33024691358024694, 'acc_stderr': 0.026168298456732846, 'acc_norm': 0.28703703703703703, 'acc_norm_stderr': 0.02517104191530968}, 'hendrycksTest-professional_law': {'acc': 0.2653194263363755, 'acc_stderr': 0.011276198843958881, 'acc_norm': 0.30182529335071706, 'acc_norm_stderr': 0.011724350518105886}, 'hendrycksTest-virology': {'acc': 0.39759036144578314, 'acc_stderr': 0.038099730845402184, 'acc_norm': 0.35542168674698793, 'acc_norm_stderr': 0.03726214354322415}, 'hendrycksTest-machine_learning': {'acc': 0.3125, 'acc_stderr': 0.043994650575715215, 'acc_norm': 0.2767857142857143, 'acc_norm_stderr': 0.04246624336697624}, 'hendrycksTest-marketing': {'acc': 0.47863247863247865, 'acc_stderr': 0.032726164476349545, 'acc_norm': 0.4358974358974359, 'acc_norm_stderr': 0.032485775115784}, 'hendrycksTest-us_foreign_policy': {'acc': 0.48, 'acc_stderr': 0.050211673156867795, 'acc_norm': 0.45, 'acc_norm_stderr': 0.05}, 'hendrycksTest-high_school_european_history': {'acc': 0.3878787878787879, 'acc_stderr': 0.03804913653971011, 'acc_norm': 0.3696969696969697, 'acc_norm_stderr': 0.03769430314512568}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.3487179487179487, 'acc_stderr': 0.024162780284017717, 'acc_norm': 0.2923076923076923, 'acc_norm_stderr': 0.023060438380857733}, 'hendrycksTest-logical_fallacies': {'acc': 0.3496932515337423, 'acc_stderr': 0.03746668325470021, 'acc_norm': 0.3558282208588957, 'acc_norm_stderr': 0.03761521380046734}, 'hendrycksTest-abstract_algebra': {'acc': 0.14, 'acc_stderr': 0.03487350880197769, 'acc_norm': 0.2, 'acc_norm_stderr': 0.04020151261036845}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.43523316062176165, 'acc_stderr': 0.035780381650085846, 'acc_norm': 0.3626943005181347, 'acc_norm_stderr': 0.03469713791704371}, 'hendrycksTest-moral_disputes': {'acc': 0.3468208092485549, 'acc_stderr': 0.02562472399403046, 'acc_norm': 0.3352601156069364, 'acc_norm_stderr': 0.025416003773165552}, 'hendrycksTest-high_school_world_history': {'acc': 0.4008438818565401, 'acc_stderr': 0.031900803894732356, 'acc_norm': 0.3628691983122363, 'acc_norm_stderr': 0.03129920825530213}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.29831932773109243, 'acc_stderr': 0.029719142876342863, 'acc_norm': 0.38235294117647056, 'acc_norm_stderr': 0.03156663099215416}, 'hendrycksTest-college_medicine': {'acc': 0.2774566473988439, 'acc_stderr': 0.034140140070440354, 'acc_norm': 0.2774566473988439, 'acc_norm_stderr': 0.034140140070440354}, 'hendrycksTest-security_studies': {'acc': 0.4448979591836735, 'acc_stderr': 0.031814251181977865, 'acc_norm': 0.3510204081632653, 'acc_norm_stderr': 0.03055531675557364}, 'hendrycksTest-high_school_geography': {'acc': 0.42424242424242425, 'acc_stderr': 0.03521224908841583, 'acc_norm': 0.31313131313131315, 'acc_norm_stderr': 0.033042050878136525}, 'hendrycksTest-world_religions': {'acc': 0.5614035087719298, 'acc_stderr': 0.03805797505590461, 'acc_norm': 0.4502923976608187, 'acc_norm_stderr': 0.03815827365913235}, 'hendrycksTest-business_ethics': {'acc': 0.43, 'acc_stderr': 0.049756985195624284, 'acc_norm': 0.38, 'acc_norm_stderr': 0.04878317312145633}, 'hendrycksTest-professional_accounting': {'acc': 0.2765957446808511, 'acc_stderr': 0.026684564340460987, 'acc_norm': 0.30141843971631205, 'acc_norm_stderr': 0.02737412888263115}, 'hendrycksTest-global_facts': {'acc': 0.29, 'acc_stderr': 0.045604802157206824, 'acc_norm': 0.23, 'acc_norm_stderr': 0.04229525846816508}, 'hendrycksTest-high_school_us_history': {'acc': 0.37745098039215685, 'acc_stderr': 0.03402272044340704, 'acc_norm': 0.3382352941176471, 'acc_norm_stderr': 0.03320574612945431}, 'hendrycksTest-professional_psychology': {'acc': 0.3284313725490196, 'acc_stderr': 0.018999707383162666, 'acc_norm': 0.3055555555555556, 'acc_norm_stderr': 0.01863559403442397}, 'hendrycksTest-college_computer_science': {'acc': 0.31, 'acc_stderr': 0.04648231987117316, 'acc_norm': 0.32, 'acc_norm_stderr': 0.046882617226215034}, 'hendrycksTest-philosophy': {'acc': 0.33440514469453375, 'acc_stderr': 0.026795422327893947, 'acc_norm': 0.33440514469453375, 'acc_norm_stderr': 0.02679542232789395}, 'hendrycksTest-anatomy': {'acc': 0.37777777777777777, 'acc_stderr': 0.04188307537595853, 'acc_norm': 0.2518518518518518, 'acc_norm_stderr': 0.03749850709174023}, 'hendrycksTest-nutrition': {'acc': 0.41830065359477125, 'acc_stderr': 0.02824513402438728, 'acc_norm': 0.42810457516339867, 'acc_norm_stderr': 0.028332397483664264}, 'hendrycksTest-computer_security': {'acc': 0.46, 'acc_stderr': 0.05009082659620333, 'acc_norm': 0.42, 'acc_norm_stderr': 0.049604496374885836}, 'hendrycksTest-econometrics': {'acc': 0.2982456140350877, 'acc_stderr': 0.04303684033537315, 'acc_norm': 0.2543859649122807, 'acc_norm_stderr': 0.040969851398436716}, 'hendrycksTest-clinical_knowledge': {'acc': 0.32452830188679244, 'acc_stderr': 0.028815615713432115, 'acc_norm': 0.3584905660377358, 'acc_norm_stderr': 0.029514703583981755}, 'hendrycksTest-high_school_mathematics': {'acc': 0.18888888888888888, 'acc_stderr': 0.023865318862285323, 'acc_norm': 0.26666666666666666, 'acc_norm_stderr': 0.026962424325073824}, 'hendrycksTest-medical_genetics': {'acc': 0.35, 'acc_stderr': 0.047937248544110196, 'acc_norm': 0.41, 'acc_norm_stderr': 0.049431107042371025}, 'hendrycksTest-public_relations': {'acc': 0.38181818181818183, 'acc_stderr': 0.04653429807913508, 'acc_norm': 0.22727272727272727, 'acc_norm_stderr': 0.040139645540727735}, 'hendrycksTest-miscellaneous': {'acc': 0.4521072796934866, 'acc_stderr': 0.01779775149386563, 'acc_norm': 0.3282247765006386, 'acc_norm_stderr': 0.01679168564019289}, 'hendrycksTest-management': {'acc': 0.44660194174757284, 'acc_stderr': 0.04922424153458935, 'acc_norm': 0.3592233009708738, 'acc_norm_stderr': 0.047504583990416946}, 'hendrycksTest-professional_medicine': {'acc': 0.3382352941176471, 'acc_stderr': 0.028739328513983576, 'acc_norm': 0.29411764705882354, 'acc_norm_stderr': 0.0276784686421447}, 'hendrycksTest-high_school_computer_science': {'acc': 0.34, 'acc_stderr': 0.047609522856952365, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117316}, 'hendrycksTest-high_school_psychology': {'acc': 0.363302752293578, 'acc_stderr': 0.020620603919625804, 'acc_norm': 0.28623853211009176, 'acc_norm_stderr': 0.019379436628919968}, 'hendrycksTest-high_school_chemistry': {'acc': 0.24630541871921183, 'acc_stderr': 0.03031509928561773, 'acc_norm': 0.3103448275862069, 'acc_norm_stderr': 0.03255086769970103}, 'hendrycksTest-college_mathematics': {'acc': 0.26, 'acc_stderr': 0.044084400227680794, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-high_school_statistics': {'acc': 0.25925925925925924, 'acc_stderr': 0.02988691054762696, 'acc_norm': 0.3148148148148148, 'acc_norm_stderr': 0.03167468706828979}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model': <models.LMClass.LMClass object at 0x7fd2db1ac890>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 15:05:10 root] (main.py 182): INFO Average accuracy 0.3053 - STEM
[2025-03-27 15:05:10 root] (main.py 182): INFO Average accuracy 0.3589 - humanities
[2025-03-27 15:05:10 root] (main.py 182): INFO Average accuracy 0.3889 - social sciences
[2025-03-27 15:05:10 root] (main.py 182): INFO Average accuracy 0.3691 - other (business, health, misc.)
[2025-03-27 15:05:10 root] (main.py 184): INFO Average accuracy: 0.3508
