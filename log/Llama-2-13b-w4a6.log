nohup: ignoring input
/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/workspace/AutoGPTQ-bugfix/auto_gptq/nn_modules/triton_utils/kernels.py:357: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/AutoGPTQ-bugfix/auto_gptq/nn_modules/triton_utils/kernels.py:365: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/AutoGPTQ-bugfix/auto_gptq/nn_modules/triton_utils/kernels.py:397: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
['main.py', '--model', '/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', '--eval_ppl', '--epochs', '10', '--output_dir', './log/Llama-2-13b-w4a6', '--wbits', '4', '--abits', '6', '--lwc', '--let', '--aug_loss', '--tasks', 'piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande']
[2025-03-25 14:45:23 root](main.py 258): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log/Llama-2-13b-w4a6', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=True, lwc=True, aug_loss=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  2.09it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  3.08it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]
vocab size:  32000
[2025-03-25 14:45:24 root](main.py 324): INFO === start quantization ===
/workspace/volume/yangzhe/OmniQuant/main.py:343: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  act_scales = torch.load(args.act_scales)
/workspace/volume/yangzhe/OmniQuant/main.py:344: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  act_shifts = torch.load(args.act_shifts)
get_wikitext2
[2025-03-25 14:46:48 root](omniquant.py 50): INFO Starting ...
[2025-03-25 14:47:00 root](omniquant.py 193): INFO === Start quantize layer 0 ===
/workspace/volume/yangzhe/OmniQuant/quantize/omniquant.py:211: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/workspace/volume/yangzhe/OmniQuant/utils.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
/workspace/volume/yangzhe/OmniQuant/quantize/omniquant.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with traincast():
[2025-03-25 14:47:22 root](omniquant.py 274): INFO layer 0 iter 0 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:47:40 root](omniquant.py 274): INFO layer 0 iter 1 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:47:57 root](omniquant.py 274): INFO layer 0 iter 2 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:48:15 root](omniquant.py 274): INFO layer 0 iter 3 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:48:32 root](omniquant.py 274): INFO layer 0 iter 4 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:48:50 root](omniquant.py 274): INFO layer 0 iter 5 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:49:07 root](omniquant.py 274): INFO layer 0 iter 6 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:49:25 root](omniquant.py 274): INFO layer 0 iter 7 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:49:43 root](omniquant.py 274): INFO layer 0 iter 8 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:50:00 root](omniquant.py 274): INFO layer 0 iter 9 loss:0.00010113499592989683 norm:nan max memory_allocated 19634.7880859375 
/workspace/volume/yangzhe/OmniQuant/quantize/omniquant.py:284: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with traincast():
[2025-03-25 14:50:03 root](omniquant.py 193): INFO === Start quantize layer 1 ===
/workspace/volume/yangzhe/OmniQuant/quantize/omniquant.py:211: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
[2025-03-25 14:50:30 root](omniquant.py 274): INFO layer 1 iter 0 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:50:47 root](omniquant.py 274): INFO layer 1 iter 1 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:51:05 root](omniquant.py 274): INFO layer 1 iter 2 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:51:23 root](omniquant.py 274): INFO layer 1 iter 3 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:51:40 root](omniquant.py 274): INFO layer 1 iter 4 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:51:58 root](omniquant.py 274): INFO layer 1 iter 5 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:52:16 root](omniquant.py 274): INFO layer 1 iter 6 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:52:33 root](omniquant.py 274): INFO layer 1 iter 7 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:52:51 root](omniquant.py 274): INFO layer 1 iter 8 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:53:09 root](omniquant.py 274): INFO layer 1 iter 9 loss:0.00033518625423312187 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:53:11 root](omniquant.py 193): INFO === Start quantize layer 2 ===
[2025-03-25 14:53:33 root](omniquant.py 274): INFO layer 2 iter 0 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:53:51 root](omniquant.py 274): INFO layer 2 iter 1 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:54:08 root](omniquant.py 274): INFO layer 2 iter 2 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:54:26 root](omniquant.py 274): INFO layer 2 iter 3 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:54:44 root](omniquant.py 274): INFO layer 2 iter 4 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:55:01 root](omniquant.py 274): INFO layer 2 iter 5 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:55:19 root](omniquant.py 274): INFO layer 2 iter 6 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:55:37 root](omniquant.py 274): INFO layer 2 iter 7 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:55:54 root](omniquant.py 274): INFO layer 2 iter 8 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:56:12 root](omniquant.py 274): INFO layer 2 iter 9 loss:0.0006936280988156796 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:56:15 root](omniquant.py 193): INFO === Start quantize layer 3 ===
[2025-03-25 14:56:37 root](omniquant.py 274): INFO layer 3 iter 0 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:56:54 root](omniquant.py 274): INFO layer 3 iter 1 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:57:12 root](omniquant.py 274): INFO layer 3 iter 2 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:57:29 root](omniquant.py 274): INFO layer 3 iter 3 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:57:47 root](omniquant.py 274): INFO layer 3 iter 4 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:58:05 root](omniquant.py 274): INFO layer 3 iter 5 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:58:23 root](omniquant.py 274): INFO layer 3 iter 6 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:58:40 root](omniquant.py 274): INFO layer 3 iter 7 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:58:58 root](omniquant.py 274): INFO layer 3 iter 8 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:59:16 root](omniquant.py 274): INFO layer 3 iter 9 loss:0.04413556680083275 norm:nan max memory_allocated 19634.7880859375 
[2025-03-25 14:59:19 root](omniquant.py 193): INFO === Start quantize layer 4 ===
[2025-03-25 14:59:40 root](omniquant.py 274): INFO layer 4 iter 0 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 14:59:58 root](omniquant.py 274): INFO layer 4 iter 1 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:00:16 root](omniquant.py 274): INFO layer 4 iter 2 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:00:33 root](omniquant.py 274): INFO layer 4 iter 3 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:00:51 root](omniquant.py 274): INFO layer 4 iter 4 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:01:09 root](omniquant.py 274): INFO layer 4 iter 5 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:01:26 root](omniquant.py 274): INFO layer 4 iter 6 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:01:44 root](omniquant.py 274): INFO layer 4 iter 7 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:02:02 root](omniquant.py 274): INFO layer 4 iter 8 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:02:20 root](omniquant.py 274): INFO layer 4 iter 9 loss:0.030804157257080078 norm:nan max memory_allocated 19635.0693359375 
[2025-03-25 15:02:22 root](omniquant.py 193): INFO === Start quantize layer 5 ===
[2025-03-25 15:02:44 root](omniquant.py 274): INFO layer 5 iter 0 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:03:02 root](omniquant.py 274): INFO layer 5 iter 1 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:03:20 root](omniquant.py 274): INFO layer 5 iter 2 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:03:37 root](omniquant.py 274): INFO layer 5 iter 3 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:03:55 root](omniquant.py 274): INFO layer 5 iter 4 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:04:13 root](omniquant.py 274): INFO layer 5 iter 5 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:04:30 root](omniquant.py 274): INFO layer 5 iter 6 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:04:48 root](omniquant.py 274): INFO layer 5 iter 7 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:05:06 root](omniquant.py 274): INFO layer 5 iter 8 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:05:24 root](omniquant.py 274): INFO layer 5 iter 9 loss:0.0327344611287117 norm:nan max memory_allocated 19638.1396484375 
[2025-03-25 15:05:26 root](omniquant.py 193): INFO === Start quantize layer 6 ===
[2025-03-25 15:05:48 root](omniquant.py 274): INFO layer 6 iter 0 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:06:06 root](omniquant.py 274): INFO layer 6 iter 1 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:06:24 root](omniquant.py 274): INFO layer 6 iter 2 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:06:41 root](omniquant.py 274): INFO layer 6 iter 3 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:06:59 root](omniquant.py 274): INFO layer 6 iter 4 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:07:17 root](omniquant.py 274): INFO layer 6 iter 5 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:07:35 root](omniquant.py 274): INFO layer 6 iter 6 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:07:52 root](omniquant.py 274): INFO layer 6 iter 7 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:08:10 root](omniquant.py 274): INFO layer 6 iter 8 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:08:28 root](omniquant.py 274): INFO layer 6 iter 9 loss:0.03537358343601227 norm:nan max memory_allocated 19638.2099609375 
[2025-03-25 15:08:31 root](omniquant.py 193): INFO === Start quantize layer 7 ===
[2025-03-25 15:08:52 root](omniquant.py 274): INFO layer 7 iter 0 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:09:10 root](omniquant.py 274): INFO layer 7 iter 1 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:09:28 root](omniquant.py 274): INFO layer 7 iter 2 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:09:45 root](omniquant.py 274): INFO layer 7 iter 3 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:10:03 root](omniquant.py 274): INFO layer 7 iter 4 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:10:21 root](omniquant.py 274): INFO layer 7 iter 5 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:10:38 root](omniquant.py 274): INFO layer 7 iter 6 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:10:56 root](omniquant.py 274): INFO layer 7 iter 7 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:11:14 root](omniquant.py 274): INFO layer 7 iter 8 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:11:32 root](omniquant.py 274): INFO layer 7 iter 9 loss:0.04023885726928711 norm:nan max memory_allocated 19638.2802734375 
[2025-03-25 15:11:35 root](omniquant.py 193): INFO === Start quantize layer 8 ===
[2025-03-25 15:11:56 root](omniquant.py 274): INFO layer 8 iter 0 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:12:14 root](omniquant.py 274): INFO layer 8 iter 1 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:12:31 root](omniquant.py 274): INFO layer 8 iter 2 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:12:49 root](omniquant.py 274): INFO layer 8 iter 3 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:13:07 root](omniquant.py 274): INFO layer 8 iter 4 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:13:25 root](omniquant.py 274): INFO layer 8 iter 5 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:13:42 root](omniquant.py 274): INFO layer 8 iter 6 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:14:00 root](omniquant.py 274): INFO layer 8 iter 7 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:14:17 root](omniquant.py 274): INFO layer 8 iter 8 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:14:35 root](omniquant.py 274): INFO layer 8 iter 9 loss:0.04427372291684151 norm:nan max memory_allocated 19639.3505859375 
[2025-03-25 15:14:38 root](omniquant.py 193): INFO === Start quantize layer 9 ===
[2025-03-25 15:15:00 root](omniquant.py 274): INFO layer 9 iter 0 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:15:17 root](omniquant.py 274): INFO layer 9 iter 1 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:15:35 root](omniquant.py 274): INFO layer 9 iter 2 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:15:53 root](omniquant.py 274): INFO layer 9 iter 3 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:16:10 root](omniquant.py 274): INFO layer 9 iter 4 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:16:28 root](omniquant.py 274): INFO layer 9 iter 5 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:16:46 root](omniquant.py 274): INFO layer 9 iter 6 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:17:04 root](omniquant.py 274): INFO layer 9 iter 7 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:17:21 root](omniquant.py 274): INFO layer 9 iter 8 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:17:39 root](omniquant.py 274): INFO layer 9 iter 9 loss:0.05284648388624191 norm:nan max memory_allocated 19642.4208984375 
[2025-03-25 15:17:42 root](omniquant.py 193): INFO === Start quantize layer 10 ===
[2025-03-25 15:18:14 root](omniquant.py 274): INFO layer 10 iter 0 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:18:32 root](omniquant.py 274): INFO layer 10 iter 1 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:18:50 root](omniquant.py 274): INFO layer 10 iter 2 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:19:08 root](omniquant.py 274): INFO layer 10 iter 3 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:19:25 root](omniquant.py 274): INFO layer 10 iter 4 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:19:43 root](omniquant.py 274): INFO layer 10 iter 5 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:20:01 root](omniquant.py 274): INFO layer 10 iter 6 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:20:19 root](omniquant.py 274): INFO layer 10 iter 7 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:20:36 root](omniquant.py 274): INFO layer 10 iter 8 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:20:54 root](omniquant.py 274): INFO layer 10 iter 9 loss:0.05806872993707657 norm:nan max memory_allocated 19642.4912109375 
[2025-03-25 15:20:57 root](omniquant.py 193): INFO === Start quantize layer 11 ===
[2025-03-25 15:21:32 root](omniquant.py 274): INFO layer 11 iter 0 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:21:49 root](omniquant.py 274): INFO layer 11 iter 1 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:22:07 root](omniquant.py 274): INFO layer 11 iter 2 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:22:25 root](omniquant.py 274): INFO layer 11 iter 3 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:22:42 root](omniquant.py 274): INFO layer 11 iter 4 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:23:00 root](omniquant.py 274): INFO layer 11 iter 5 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:23:18 root](omniquant.py 274): INFO layer 11 iter 6 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:23:35 root](omniquant.py 274): INFO layer 11 iter 7 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:23:53 root](omniquant.py 274): INFO layer 11 iter 8 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:24:11 root](omniquant.py 274): INFO layer 11 iter 9 loss:0.0630338191986084 norm:nan max memory_allocated 19642.5615234375 
[2025-03-25 15:24:14 root](omniquant.py 193): INFO === Start quantize layer 12 ===
[2025-03-25 15:24:47 root](omniquant.py 274): INFO layer 12 iter 0 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:25:05 root](omniquant.py 274): INFO layer 12 iter 1 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:25:22 root](omniquant.py 274): INFO layer 12 iter 2 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:25:40 root](omniquant.py 274): INFO layer 12 iter 3 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:25:58 root](omniquant.py 274): INFO layer 12 iter 4 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:26:15 root](omniquant.py 274): INFO layer 12 iter 5 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:26:33 root](omniquant.py 274): INFO layer 12 iter 6 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:26:51 root](omniquant.py 274): INFO layer 12 iter 7 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:27:09 root](omniquant.py 274): INFO layer 12 iter 8 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:27:26 root](omniquant.py 274): INFO layer 12 iter 9 loss:0.06971929222345352 norm:nan max memory_allocated 19643.6318359375 
[2025-03-25 15:27:29 root](omniquant.py 193): INFO === Start quantize layer 13 ===
[2025-03-25 15:27:55 root](omniquant.py 274): INFO layer 13 iter 0 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:28:13 root](omniquant.py 274): INFO layer 13 iter 1 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:28:30 root](omniquant.py 274): INFO layer 13 iter 2 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:28:48 root](omniquant.py 274): INFO layer 13 iter 3 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:29:06 root](omniquant.py 274): INFO layer 13 iter 4 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:29:24 root](omniquant.py 274): INFO layer 13 iter 5 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:29:41 root](omniquant.py 274): INFO layer 13 iter 6 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:29:59 root](omniquant.py 274): INFO layer 13 iter 7 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:30:17 root](omniquant.py 274): INFO layer 13 iter 8 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:30:35 root](omniquant.py 274): INFO layer 13 iter 9 loss:0.07713507860898972 norm:nan max memory_allocated 19646.7021484375 
[2025-03-25 15:30:38 root](omniquant.py 193): INFO === Start quantize layer 14 ===
[2025-03-25 15:31:00 root](omniquant.py 274): INFO layer 14 iter 0 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:31:17 root](omniquant.py 274): INFO layer 14 iter 1 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:31:35 root](omniquant.py 274): INFO layer 14 iter 2 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:31:53 root](omniquant.py 274): INFO layer 14 iter 3 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:32:10 root](omniquant.py 274): INFO layer 14 iter 4 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:32:28 root](omniquant.py 274): INFO layer 14 iter 5 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:32:46 root](omniquant.py 274): INFO layer 14 iter 6 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:33:04 root](omniquant.py 274): INFO layer 14 iter 7 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:33:21 root](omniquant.py 274): INFO layer 14 iter 8 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:33:39 root](omniquant.py 274): INFO layer 14 iter 9 loss:0.08853302150964737 norm:nan max memory_allocated 19646.7724609375 
[2025-03-25 15:33:42 root](omniquant.py 193): INFO === Start quantize layer 15 ===
[2025-03-25 15:34:04 root](omniquant.py 274): INFO layer 15 iter 0 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:34:22 root](omniquant.py 274): INFO layer 15 iter 1 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:34:39 root](omniquant.py 274): INFO layer 15 iter 2 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:34:57 root](omniquant.py 274): INFO layer 15 iter 3 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:35:15 root](omniquant.py 274): INFO layer 15 iter 4 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:35:33 root](omniquant.py 274): INFO layer 15 iter 5 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:35:50 root](omniquant.py 274): INFO layer 15 iter 6 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:36:08 root](omniquant.py 274): INFO layer 15 iter 7 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:36:26 root](omniquant.py 274): INFO layer 15 iter 8 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:36:43 root](omniquant.py 274): INFO layer 15 iter 9 loss:0.09665638208389282 norm:nan max memory_allocated 19646.8427734375 
[2025-03-25 15:36:46 root](omniquant.py 193): INFO === Start quantize layer 16 ===
[2025-03-25 15:37:08 root](omniquant.py 274): INFO layer 16 iter 0 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:37:26 root](omniquant.py 274): INFO layer 16 iter 1 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:37:43 root](omniquant.py 274): INFO layer 16 iter 2 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:38:01 root](omniquant.py 274): INFO layer 16 iter 3 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:38:19 root](omniquant.py 274): INFO layer 16 iter 4 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:38:37 root](omniquant.py 274): INFO layer 16 iter 5 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:38:54 root](omniquant.py 274): INFO layer 16 iter 6 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:39:12 root](omniquant.py 274): INFO layer 16 iter 7 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:39:30 root](omniquant.py 274): INFO layer 16 iter 8 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:39:48 root](omniquant.py 274): INFO layer 16 iter 9 loss:0.11139756441116333 norm:nan max memory_allocated 19647.9130859375 
[2025-03-25 15:39:50 root](omniquant.py 193): INFO === Start quantize layer 17 ===
[2025-03-25 15:40:12 root](omniquant.py 274): INFO layer 17 iter 0 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:40:30 root](omniquant.py 274): INFO layer 17 iter 1 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:40:47 root](omniquant.py 274): INFO layer 17 iter 2 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:41:05 root](omniquant.py 274): INFO layer 17 iter 3 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:41:23 root](omniquant.py 274): INFO layer 17 iter 4 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:41:41 root](omniquant.py 274): INFO layer 17 iter 5 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:41:58 root](omniquant.py 274): INFO layer 17 iter 6 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:42:16 root](omniquant.py 274): INFO layer 17 iter 7 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:42:34 root](omniquant.py 274): INFO layer 17 iter 8 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:42:52 root](omniquant.py 274): INFO layer 17 iter 9 loss:0.1221984326839447 norm:nan max memory_allocated 19650.9833984375 
[2025-03-25 15:42:54 root](omniquant.py 193): INFO === Start quantize layer 18 ===
[2025-03-25 15:43:16 root](omniquant.py 274): INFO layer 18 iter 0 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:43:34 root](omniquant.py 274): INFO layer 18 iter 1 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:43:52 root](omniquant.py 274): INFO layer 18 iter 2 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:44:09 root](omniquant.py 274): INFO layer 18 iter 3 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:44:27 root](omniquant.py 274): INFO layer 18 iter 4 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:44:45 root](omniquant.py 274): INFO layer 18 iter 5 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:45:03 root](omniquant.py 274): INFO layer 18 iter 6 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:45:20 root](omniquant.py 274): INFO layer 18 iter 7 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:45:38 root](omniquant.py 274): INFO layer 18 iter 8 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:45:56 root](omniquant.py 274): INFO layer 18 iter 9 loss:0.13714216649532318 norm:nan max memory_allocated 19651.0537109375 
[2025-03-25 15:45:59 root](omniquant.py 193): INFO === Start quantize layer 19 ===
[2025-03-25 15:46:20 root](omniquant.py 274): INFO layer 19 iter 0 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:46:38 root](omniquant.py 274): INFO layer 19 iter 1 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:46:56 root](omniquant.py 274): INFO layer 19 iter 2 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:47:14 root](omniquant.py 274): INFO layer 19 iter 3 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:47:31 root](omniquant.py 274): INFO layer 19 iter 4 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:47:49 root](omniquant.py 274): INFO layer 19 iter 5 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:48:07 root](omniquant.py 274): INFO layer 19 iter 6 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:48:25 root](omniquant.py 274): INFO layer 19 iter 7 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:48:42 root](omniquant.py 274): INFO layer 19 iter 8 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:49:00 root](omniquant.py 274): INFO layer 19 iter 9 loss:0.15376923978328705 norm:nan max memory_allocated 19651.1240234375 
[2025-03-25 15:49:03 root](omniquant.py 193): INFO === Start quantize layer 20 ===
[2025-03-25 15:49:24 root](omniquant.py 274): INFO layer 20 iter 0 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:49:42 root](omniquant.py 274): INFO layer 20 iter 1 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:50:00 root](omniquant.py 274): INFO layer 20 iter 2 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:50:18 root](omniquant.py 274): INFO layer 20 iter 3 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:50:35 root](omniquant.py 274): INFO layer 20 iter 4 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:50:53 root](omniquant.py 274): INFO layer 20 iter 5 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:51:11 root](omniquant.py 274): INFO layer 20 iter 6 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:51:28 root](omniquant.py 274): INFO layer 20 iter 7 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:51:46 root](omniquant.py 274): INFO layer 20 iter 8 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:52:04 root](omniquant.py 274): INFO layer 20 iter 9 loss:0.17285236716270447 norm:nan max memory_allocated 19652.1943359375 
[2025-03-25 15:52:07 root](omniquant.py 193): INFO === Start quantize layer 21 ===
[2025-03-25 15:52:28 root](omniquant.py 274): INFO layer 21 iter 0 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:52:46 root](omniquant.py 274): INFO layer 21 iter 1 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:53:04 root](omniquant.py 274): INFO layer 21 iter 2 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:53:22 root](omniquant.py 274): INFO layer 21 iter 3 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:53:39 root](omniquant.py 274): INFO layer 21 iter 4 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:53:57 root](omniquant.py 274): INFO layer 21 iter 5 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:54:15 root](omniquant.py 274): INFO layer 21 iter 6 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:54:32 root](omniquant.py 274): INFO layer 21 iter 7 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:54:50 root](omniquant.py 274): INFO layer 21 iter 8 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:55:08 root](omniquant.py 274): INFO layer 21 iter 9 loss:0.20509260892868042 norm:nan max memory_allocated 19655.2646484375 
[2025-03-25 15:55:11 root](omniquant.py 193): INFO === Start quantize layer 22 ===
[2025-03-25 15:55:32 root](omniquant.py 274): INFO layer 22 iter 0 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:55:50 root](omniquant.py 274): INFO layer 22 iter 1 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:56:08 root](omniquant.py 274): INFO layer 22 iter 2 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:56:25 root](omniquant.py 274): INFO layer 22 iter 3 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:56:43 root](omniquant.py 274): INFO layer 22 iter 4 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:57:01 root](omniquant.py 274): INFO layer 22 iter 5 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:57:19 root](omniquant.py 274): INFO layer 22 iter 6 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:57:36 root](omniquant.py 274): INFO layer 22 iter 7 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:57:54 root](omniquant.py 274): INFO layer 22 iter 8 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:58:12 root](omniquant.py 274): INFO layer 22 iter 9 loss:0.23184755444526672 norm:nan max memory_allocated 19655.3349609375 
[2025-03-25 15:58:15 root](omniquant.py 193): INFO === Start quantize layer 23 ===
[2025-03-25 15:58:36 root](omniquant.py 274): INFO layer 23 iter 0 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 15:58:54 root](omniquant.py 274): INFO layer 23 iter 1 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 15:59:12 root](omniquant.py 274): INFO layer 23 iter 2 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 15:59:30 root](omniquant.py 274): INFO layer 23 iter 3 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 15:59:47 root](omniquant.py 274): INFO layer 23 iter 4 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 16:00:05 root](omniquant.py 274): INFO layer 23 iter 5 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 16:00:23 root](omniquant.py 274): INFO layer 23 iter 6 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 16:00:41 root](omniquant.py 274): INFO layer 23 iter 7 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 16:00:58 root](omniquant.py 274): INFO layer 23 iter 8 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 16:01:16 root](omniquant.py 274): INFO layer 23 iter 9 loss:0.2633764445781708 norm:nan max memory_allocated 19655.4052734375 
[2025-03-25 16:01:19 root](omniquant.py 193): INFO === Start quantize layer 24 ===
[2025-03-25 16:01:40 root](omniquant.py 274): INFO layer 24 iter 0 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:01:58 root](omniquant.py 274): INFO layer 24 iter 1 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:02:16 root](omniquant.py 274): INFO layer 24 iter 2 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:02:34 root](omniquant.py 274): INFO layer 24 iter 3 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:02:51 root](omniquant.py 274): INFO layer 24 iter 4 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:03:09 root](omniquant.py 274): INFO layer 24 iter 5 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:03:27 root](omniquant.py 274): INFO layer 24 iter 6 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:03:45 root](omniquant.py 274): INFO layer 24 iter 7 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:04:02 root](omniquant.py 274): INFO layer 24 iter 8 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:04:20 root](omniquant.py 274): INFO layer 24 iter 9 loss:0.2978942096233368 norm:nan max memory_allocated 19656.4755859375 
[2025-03-25 16:04:23 root](omniquant.py 193): INFO === Start quantize layer 25 ===
[2025-03-25 16:04:45 root](omniquant.py 274): INFO layer 25 iter 0 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:05:03 root](omniquant.py 274): INFO layer 25 iter 1 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:05:20 root](omniquant.py 274): INFO layer 25 iter 2 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:05:38 root](omniquant.py 274): INFO layer 25 iter 3 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:05:56 root](omniquant.py 274): INFO layer 25 iter 4 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:06:14 root](omniquant.py 274): INFO layer 25 iter 5 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:06:31 root](omniquant.py 274): INFO layer 25 iter 6 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:06:49 root](omniquant.py 274): INFO layer 25 iter 7 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:07:07 root](omniquant.py 274): INFO layer 25 iter 8 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:07:24 root](omniquant.py 274): INFO layer 25 iter 9 loss:0.3354519009590149 norm:nan max memory_allocated 19659.5458984375 
[2025-03-25 16:07:27 root](omniquant.py 193): INFO === Start quantize layer 26 ===
[2025-03-25 16:07:49 root](omniquant.py 274): INFO layer 26 iter 0 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:08:07 root](omniquant.py 274): INFO layer 26 iter 1 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:08:25 root](omniquant.py 274): INFO layer 26 iter 2 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:08:42 root](omniquant.py 274): INFO layer 26 iter 3 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:09:00 root](omniquant.py 274): INFO layer 26 iter 4 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:09:18 root](omniquant.py 274): INFO layer 26 iter 5 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:09:36 root](omniquant.py 274): INFO layer 26 iter 6 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:09:53 root](omniquant.py 274): INFO layer 26 iter 7 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:10:11 root](omniquant.py 274): INFO layer 26 iter 8 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:10:29 root](omniquant.py 274): INFO layer 26 iter 9 loss:0.3820660710334778 norm:nan max memory_allocated 19659.6162109375 
[2025-03-25 16:10:32 root](omniquant.py 193): INFO === Start quantize layer 27 ===
[2025-03-25 16:10:53 root](omniquant.py 274): INFO layer 27 iter 0 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:11:11 root](omniquant.py 274): INFO layer 27 iter 1 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:11:29 root](omniquant.py 274): INFO layer 27 iter 2 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:11:47 root](omniquant.py 274): INFO layer 27 iter 3 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:12:04 root](omniquant.py 274): INFO layer 27 iter 4 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:12:22 root](omniquant.py 274): INFO layer 27 iter 5 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:12:40 root](omniquant.py 274): INFO layer 27 iter 6 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:12:58 root](omniquant.py 274): INFO layer 27 iter 7 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:13:16 root](omniquant.py 274): INFO layer 27 iter 8 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:13:33 root](omniquant.py 274): INFO layer 27 iter 9 loss:0.41979601979255676 norm:nan max memory_allocated 19659.6865234375 
[2025-03-25 16:13:36 root](omniquant.py 193): INFO === Start quantize layer 28 ===
[2025-03-25 16:13:58 root](omniquant.py 274): INFO layer 28 iter 0 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:14:16 root](omniquant.py 274): INFO layer 28 iter 1 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:14:33 root](omniquant.py 274): INFO layer 28 iter 2 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:14:51 root](omniquant.py 274): INFO layer 28 iter 3 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:15:09 root](omniquant.py 274): INFO layer 28 iter 4 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:15:27 root](omniquant.py 274): INFO layer 28 iter 5 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:15:45 root](omniquant.py 274): INFO layer 28 iter 6 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:16:02 root](omniquant.py 274): INFO layer 28 iter 7 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:16:20 root](omniquant.py 274): INFO layer 28 iter 8 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:16:38 root](omniquant.py 274): INFO layer 28 iter 9 loss:0.47271639108657837 norm:nan max memory_allocated 19660.7568359375 
[2025-03-25 16:16:41 root](omniquant.py 193): INFO === Start quantize layer 29 ===
[2025-03-25 16:17:02 root](omniquant.py 274): INFO layer 29 iter 0 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:17:20 root](omniquant.py 274): INFO layer 29 iter 1 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:17:38 root](omniquant.py 274): INFO layer 29 iter 2 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:17:56 root](omniquant.py 274): INFO layer 29 iter 3 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:18:14 root](omniquant.py 274): INFO layer 29 iter 4 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:18:31 root](omniquant.py 274): INFO layer 29 iter 5 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:18:49 root](omniquant.py 274): INFO layer 29 iter 6 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:19:07 root](omniquant.py 274): INFO layer 29 iter 7 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:19:25 root](omniquant.py 274): INFO layer 29 iter 8 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:19:43 root](omniquant.py 274): INFO layer 29 iter 9 loss:0.5204770565032959 norm:nan max memory_allocated 19663.8271484375 
[2025-03-25 16:19:45 root](omniquant.py 193): INFO === Start quantize layer 30 ===
[2025-03-25 16:20:07 root](omniquant.py 274): INFO layer 30 iter 0 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:20:25 root](omniquant.py 274): INFO layer 30 iter 1 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:20:43 root](omniquant.py 274): INFO layer 30 iter 2 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:21:01 root](omniquant.py 274): INFO layer 30 iter 3 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:21:18 root](omniquant.py 274): INFO layer 30 iter 4 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:21:36 root](omniquant.py 274): INFO layer 30 iter 5 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:21:54 root](omniquant.py 274): INFO layer 30 iter 6 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:22:12 root](omniquant.py 274): INFO layer 30 iter 7 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:22:30 root](omniquant.py 274): INFO layer 30 iter 8 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:22:47 root](omniquant.py 274): INFO layer 30 iter 9 loss:0.5726929903030396 norm:nan max memory_allocated 19663.8974609375 
[2025-03-25 16:22:50 root](omniquant.py 193): INFO === Start quantize layer 31 ===
[2025-03-25 16:23:12 root](omniquant.py 274): INFO layer 31 iter 0 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:23:29 root](omniquant.py 274): INFO layer 31 iter 1 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:23:47 root](omniquant.py 274): INFO layer 31 iter 2 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:24:05 root](omniquant.py 274): INFO layer 31 iter 3 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:24:23 root](omniquant.py 274): INFO layer 31 iter 4 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:24:40 root](omniquant.py 274): INFO layer 31 iter 5 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:24:58 root](omniquant.py 274): INFO layer 31 iter 6 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:25:16 root](omniquant.py 274): INFO layer 31 iter 7 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:25:34 root](omniquant.py 274): INFO layer 31 iter 8 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:25:51 root](omniquant.py 274): INFO layer 31 iter 9 loss:0.6260248422622681 norm:nan max memory_allocated 19663.9677734375 
[2025-03-25 16:25:54 root](omniquant.py 193): INFO === Start quantize layer 32 ===
[2025-03-25 16:26:16 root](omniquant.py 274): INFO layer 32 iter 0 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:26:33 root](omniquant.py 274): INFO layer 32 iter 1 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:26:51 root](omniquant.py 274): INFO layer 32 iter 2 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:27:09 root](omniquant.py 274): INFO layer 32 iter 3 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:27:27 root](omniquant.py 274): INFO layer 32 iter 4 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:27:45 root](omniquant.py 274): INFO layer 32 iter 5 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:28:02 root](omniquant.py 274): INFO layer 32 iter 6 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:28:20 root](omniquant.py 274): INFO layer 32 iter 7 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:28:38 root](omniquant.py 274): INFO layer 32 iter 8 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:28:56 root](omniquant.py 274): INFO layer 32 iter 9 loss:0.6910563707351685 norm:nan max memory_allocated 19667.0380859375 
[2025-03-25 16:28:59 root](omniquant.py 193): INFO === Start quantize layer 33 ===
[2025-03-25 16:29:20 root](omniquant.py 274): INFO layer 33 iter 0 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:29:38 root](omniquant.py 274): INFO layer 33 iter 1 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:29:56 root](omniquant.py 274): INFO layer 33 iter 2 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:30:13 root](omniquant.py 274): INFO layer 33 iter 3 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:30:31 root](omniquant.py 274): INFO layer 33 iter 4 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:30:49 root](omniquant.py 274): INFO layer 33 iter 5 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:31:07 root](omniquant.py 274): INFO layer 33 iter 6 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:31:25 root](omniquant.py 274): INFO layer 33 iter 7 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:31:42 root](omniquant.py 274): INFO layer 33 iter 8 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:32:00 root](omniquant.py 274): INFO layer 33 iter 9 loss:0.7571772933006287 norm:nan max memory_allocated 19668.1083984375 
[2025-03-25 16:32:03 root](omniquant.py 193): INFO === Start quantize layer 34 ===
[2025-03-25 16:32:25 root](omniquant.py 274): INFO layer 34 iter 0 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:32:43 root](omniquant.py 274): INFO layer 34 iter 1 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:33:00 root](omniquant.py 274): INFO layer 34 iter 2 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:33:18 root](omniquant.py 274): INFO layer 34 iter 3 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:33:36 root](omniquant.py 274): INFO layer 34 iter 4 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:33:53 root](omniquant.py 274): INFO layer 34 iter 5 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:34:11 root](omniquant.py 274): INFO layer 34 iter 6 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:34:29 root](omniquant.py 274): INFO layer 34 iter 7 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:34:47 root](omniquant.py 274): INFO layer 34 iter 8 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:35:05 root](omniquant.py 274): INFO layer 34 iter 9 loss:0.8360987305641174 norm:nan max memory_allocated 19668.1787109375 
[2025-03-25 16:35:07 root](omniquant.py 193): INFO === Start quantize layer 35 ===
[2025-03-25 16:35:29 root](omniquant.py 274): INFO layer 35 iter 0 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:35:47 root](omniquant.py 274): INFO layer 35 iter 1 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:36:05 root](omniquant.py 274): INFO layer 35 iter 2 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:36:22 root](omniquant.py 274): INFO layer 35 iter 3 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:36:40 root](omniquant.py 274): INFO layer 35 iter 4 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:36:58 root](omniquant.py 274): INFO layer 35 iter 5 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:37:15 root](omniquant.py 274): INFO layer 35 iter 6 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:37:33 root](omniquant.py 274): INFO layer 35 iter 7 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:37:51 root](omniquant.py 274): INFO layer 35 iter 8 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:38:09 root](omniquant.py 274): INFO layer 35 iter 9 loss:0.922217845916748 norm:nan max memory_allocated 19668.2490234375 
[2025-03-25 16:38:12 root](omniquant.py 193): INFO === Start quantize layer 36 ===
[2025-03-25 16:38:33 root](omniquant.py 274): INFO layer 36 iter 0 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:38:51 root](omniquant.py 274): INFO layer 36 iter 1 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:39:09 root](omniquant.py 274): INFO layer 36 iter 2 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:39:27 root](omniquant.py 274): INFO layer 36 iter 3 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:39:44 root](omniquant.py 274): INFO layer 36 iter 4 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:40:02 root](omniquant.py 274): INFO layer 36 iter 5 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:40:20 root](omniquant.py 274): INFO layer 36 iter 6 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:40:38 root](omniquant.py 274): INFO layer 36 iter 7 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:40:56 root](omniquant.py 274): INFO layer 36 iter 8 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:41:13 root](omniquant.py 274): INFO layer 36 iter 9 loss:1.0421860218048096 norm:nan max memory_allocated 19671.3193359375 
[2025-03-25 16:41:16 root](omniquant.py 193): INFO === Start quantize layer 37 ===
[2025-03-25 16:41:38 root](omniquant.py 274): INFO layer 37 iter 0 loss:1.1847999095916748 norm:nan max memory_allocated 19672.3896484375 
[2025-03-25 16:41:45 root](omniquant.py 264): INFO Loss is NAN, stopping training
> /workspace/volume/yangzhe/OmniQuant/quantize/omniquant.py(267)omniquant()
-> loss_list.append(loss.detach().cpu())
(Pdb) Traceback (most recent call last):
  File "/workspace/volume/yangzhe/OmniQuant/main.py", line 375, in <module>
    main()
  File "/workspace/volume/yangzhe/OmniQuant/main.py", line 345, in main
    omniquant(
  File "/workspace/volume/yangzhe/OmniQuant/quantize/omniquant.py", line 267, in omniquant
    loss_list.append(loss.detach().cpu())
    ^^^^^^^^^
  File "/opt/conda/lib/python3.11/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/bdb.py", line 114, in dispatch_line
    self.user_line(frame)
  File "/opt/conda/lib/python3.11/pdb.py", line 325, in user_line
    self.interaction(frame, None)
  File "/opt/conda/lib/python3.11/pdb.py", line 419, in interaction
    self._cmdloop()
  File "/opt/conda/lib/python3.11/pdb.py", line 385, in _cmdloop
    self.cmdloop()
  File "/opt/conda/lib/python3.11/cmd.py", line 126, in cmdloop
    line = input(self.prompt)
           ^^^^^^^^^^^^^^^^^^
OSError: [Errno 9] Bad file descriptor
